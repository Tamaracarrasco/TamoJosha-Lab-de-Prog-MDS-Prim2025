{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl0JoW4Eodvi"
      },
      "source": [
        "# **Laboratorio 9: Airflow üõ´**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ypG7Fsodvj"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Josefa Anselmo.\n",
        "- Nombre de alumno 2: Tamara Carrasco.\n",
        "\n",
        "### **Link de repositorio de GitHub:** [TamoJosha](https://github.com/Tamaracarrasco/TamoJosha-Lab-de-Prog-MDS-Prim2025/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_P7PCPTodvk"
      },
      "source": [
        "## Temas a tratar\n",
        "\n",
        "- Construcci√≥n de pipelines productivos usando `Airflow`.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Reconocer los componentes pricipales de `Airflow` y su funcionamiento.\n",
        "- Poner en pr√°ctica la construcci√≥n de pipelines de `Airflow`.\n",
        "- Automatizar procesos t√≠picos de un proyecto de ciencia de datos mediante `Airflow` y `Docker`.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfK981Uodvk"
      },
      "source": [
        "# **Introducci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ilM8YDjodvk"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/OBQ6niqbxswAAAAM/legallyblonde.gif\" width=\"300\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrLPQNBodvk"
      },
      "source": [
        "Vale, una estudiante del Mag√≠ster en Ciencia de Datos, se encuentra en la etapa final de sus estudios. Por un lado, est√° muy contenta por haber llegado tan lejos, pero por otro, no puede evitar sentirse inquieta. Desde que ingres√≥ a la universidad, una pregunta la ha perseguido: ¬øqu√© tan probable es que pueda ser seleccionada en los lugares donde env√≠e postulaciones para puestos de trabajo?\n",
        "\n",
        "Esta duda la mantiene en constante reflexi√≥n, especialmente porque sabe que el mercado laboral en Ciencia de Datos es competitivo y exige habilidades no solo t√©cnicas, sino tambi√©n estrat√©gicas para destacar. Sin embargo, Vale actualmente est√° completamente enfocada en terminar su tesis de mag√≠ster y ha tenido que postergar cualquier preparaci√≥n espec√≠fica para enfrentar el desaf√≠o de las postulaciones laborales.\n",
        "\n",
        "Al ver el avance y las habilidades que usted ha demostrado en el curso, Vale decidi√≥ proponerle un desaf√≠o que le permitir√° disminuir la incertidumbre sobre su futuro laboral. Inspirado en sus conocimientos, recolect√≥ un conjunto de datos que contiene informaci√≥n sobre diversos factores que influyen en las decisiones de contrataci√≥n de empresas al seleccionar entre sus postulantes. Este set de datos incluye los siguientes atributos:\n",
        "\n",
        "- Age: Edad del candidato\n",
        "- Gender: Genero del candidato. Male (0), Female (1).\n",
        "- EducationLevel: Mayor nivel educacional alcanzado por el candidato. Licenciatura Tipo 1 (1), Licenciatura Tipo 2 (2), Maestr√≠a (3), PhD. (4).\n",
        "- ExperienceYears: A√±os de experiencia profesional.\n",
        "- PreviousCompanies: Numero de compa√±√≠as donde el candidato ha trabajado anteriormente.\n",
        "- DistanceFromCompany: Distancia en kilometros entre la residencia del candidato y la compa√±√≠a donde postula.\n",
        "- InterviewScore: Puntaje obtenido en la entrevista por el candidato entre 0 a 100.\n",
        "- SkillScore: Puntaje obtenido en evaluaci√≥n de habilidades t√©cnicas por el candidato, entre 0 a 100.\n",
        "- PersonalityScore: Puntaje obtenido en pruebas de personalidad del candidato, entre 0 a 100.\n",
        "- RecruitmentStrategy: Estrategia del equipo de reclutamiento. Agresiva (1), Moderada (2), Conservadora (3).\n",
        "\n",
        "Variable a predecir:\n",
        "- HiringDecision: Resultado de la postulaci√≥n. No contratado (0), Contratado (1).\n",
        "\n",
        "Su objetivo ser√° ayudar a Vale a desarrollar un modelo que le permita predecir, basado en estos factores, si un postulante ser√° contratado o no. Esta herramienta no solo le dar√° a Vale mayor claridad sobre el impacto de ciertos atributos en la decisi√≥n final de contrataci√≥n, sino que tambi√©n le permitir√° aplicar sus conocimientos de Ciencia de Datos para resolver una pregunta que a muchos estudiantes como a ella les inquieta.\n",
        "\n",
        "Como estudiante del curso Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos, deber√° demostrar sus capacidades para preprocesar, analizar y modelar datos, brind√°ndole a Vale una soluci√≥n robusta y bien fundamentada para su problem√°tica.\n",
        "\n",
        "`Nota:` El siguiente [enlace](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data/data) contiene el set de datos original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yeh268atodvl"
      },
      "source": [
        "# **1. Pipeline de Predicci√≥n Lineal** (30 Puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmB1LTWnodvl"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.licdn.com/dms/image/v2/D4E22AQHZplrdPyKnvA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1713736729086?e=2147483647&v=beta&t=Tad2ulaWkhhDrPRN0PCdXrfuza60PjoJqgLborDyLao\" width=\"500\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF1bTY0Modvl"
      },
      "source": [
        "En esta secci√≥n buscaremos desplegar un producto utilizando un modelo de clasificaci√≥n `Random Forest` para determinar **si una persona ser√° contratada o no en un proceso de selecci√≥n**. Para ello, comenzaremos preparando un pipeline lineal mediante `Airflow`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7MllF4fodvl"
      },
      "source": [
        "## **1.1 Preparando el Pipeline** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1JxaZgModvl"
      },
      "source": [
        "**Primero, aseg√∫rese de tener creada las carpetas `dags`, `plugins` y `logs`**.\n",
        "\n",
        "Comenzamos preparando un archivo llamado `hiring_functions.py`, el cual guardar√° en la carpeta `dags` y debe contener lo siguiente:\n",
        "\n",
        "1. (3 puntos) Una funci√≥n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci√≥n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
        "  - raw\n",
        "  - splits\n",
        "  - models\n",
        "\n",
        "  `Hint`: Puede hacer uso de kwargs para obtener la fecha de ejecuci√≥n mediante el DAG. El siguiente [Enlace](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) le puede ser √∫til.\n",
        "\n",
        "2. (3 puntos) Una funci√≥n llamada `split_data()` que lea el archivo `data_1.csv` de la carepta `raw` y a partir de este, aplique un *hold out*, generando un dataset de entrenamiento y uno de prueba. Luego debe guardar estos nuevos conjuntos de datos en la carpeta `splits`. `Nota:` Utilice un 20% para el conjunto de prueba, mantenga la proporci√≥n original en la variable objetivo y fije una semilla.\n",
        "\n",
        "3. (8 puntos) Cree una funci√≥n llamada `preprocess_and_train()` que:\n",
        "  - Lea los set de entrenamiento y prueba de la carpeta `splits`.\n",
        "  - Cree y aplique un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes. Puede apoyarse del archivo `data_1_report.html` para justificar cualquier paso del preprocesamiento.\n",
        "  \n",
        "  - A√±ada una etapa de entrenamiento utilizando el modelo `RandomForest`.\n",
        "  \n",
        "  Esta funci√≥n **debe crear un archivo `joblib` (an√°logo a `pickle`) con el pipeline entrenado** en la carepta `models`, adem√°s debe **imprimir** el accuracy en el conjunto de prueba y el f1-score de la clase positiva (contratado).\n",
        "  \n",
        "3. (1 punto) Incorpore la funci√≥n `gradio_interface` en su script, modificando la ruta de acceso a su modelo, de forma que pueda leerlo desde la carepta `models`. Puede realizar las modificaciones que estime necesarias.\n",
        "\n",
        "`NOTA:` Se permite la creaci√≥n de funciones auxiliares si lo estiman conveniente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze9Iotloodvl"
      },
      "outputs": [],
      "source": [
        "#Inserte su c√≥digo aqui\n",
        "\n",
        "# Este es el c√≥digo de mi archivo hiring_functions.py\n",
        "\n",
        "## Desarrollo parte 1.1 Preparando el Pipeline\n",
        "\n",
        "\n",
        "## creaci√≥n de las carpetas\n",
        "# Primero me ubico donde quiero crearlas\n",
        "cd Laboratorios\\Laboratorio_9\n",
        "\n",
        "# y las creo --> Por temas de compatibilidad con el tipo de archivo que se puede subir a GIT, tuvimos qeu crear un gitignor que \"elimina\" de repo\n",
        "# remoto las carpetas de logs y plugins, pero en local las necesitamos, por eso se podr√≠a  crear con el comando solo esas dos y no dags por esa si permite subirlo\n",
        "mkdir -Force dags, plugins, logs\n",
        "\n",
        "# Luego creo el archivo dentro de dags\n",
        "New-Item -Path \".\\dags\\hiring_functions.py\" -ItemType \"File\" -Force\n",
        "\n",
        "\n",
        "# hiring_functions.py\n",
        "\n",
        "# Librer√≠as\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import joblib\n",
        "\n",
        "\n",
        "# Primera definici√≥n: create_folders(**context)\n",
        "\n",
        "def create_folders(**context):\n",
        "    \"\"\"\n",
        "    Crea la carpeta de ejecuci√≥n YYYY-MM-DD con subcarpetas raw/splits/models\n",
        "    \"\"\"\n",
        "    ds = context[\"ds\"]                             \n",
        "    dags_dir = Path(__file__).resolve().parent    \n",
        "    run_dir  = dags_dir / ds\n",
        "\n",
        "    for sub in [\"raw\", \"splits\", \"models\"]:\n",
        "        (run_dir / sub).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "\n",
        "# Segunda definici√≥n: split_data(**context)--> hold-out 80/20 estratificado\n",
        "\n",
        "def split_data(**context):\n",
        "    \"\"\"\n",
        "    Lee <ds>/raw/data_1.csv, aplica hold-out 80/20 estratificado (seed=42)\n",
        "    y guarda train/test en <ds>/splits.\n",
        "    \"\"\"\n",
        "    ds = context[\"ds\"]\n",
        "    base = Path(__file__).resolve().parent / ds\n",
        "    raw_file = base / \"raw\" / \"data_1.csv\"\n",
        "\n",
        "    df = pd.read_csv(raw_file)\n",
        "\n",
        "    y = df[\"HiringDecision\"]\n",
        "    X = df.drop(columns=[\"HiringDecision\"])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    splits_dir = base / \"splits\"\n",
        "    splits_dir.mkdir(parents=True, exist_ok=True)\n",
        "    X_train.assign(HiringDecision=y_train).to_csv(splits_dir / \"train.csv\", index=False)\n",
        "    X_test.assign(HiringDecision=y_test).to_csv(splits_dir / \"test.csv\", index=False)\n",
        "\n",
        "    print(f\"[split_data] Guardados: {splits_dir / 'train.csv'} y {splits_dir / 'test.csv'}\")\n",
        "\n",
        "\n",
        "# Tercera definici√≥n: preprocess_and_train(**context) --> preprocesa, entrena y eval√∫a\n",
        "\n",
        "def preprocess_and_train(**context):\n",
        "    \"\"\"\n",
        "    - Lee <ds>/splits/train.csv y test.csv\n",
        "    - Preprocesa: imputaci√≥n median (num) / most_frequent + OneHot (cat)\n",
        "    - Entrena RandomForest\n",
        "    - Imprime accuracy y f1 (clase positiva=1)\n",
        "    - Guarda pipeline en <ds>/models/pipeline.joblib\n",
        "    \"\"\"\n",
        "    ds = context[\"ds\"]\n",
        "    base = Path(__file__).resolve().parent / ds\n",
        "    splits_dir = base / \"splits\"\n",
        "    models_dir = base / \"models\"\n",
        "\n",
        "    train_df = pd.read_csv(splits_dir / \"train.csv\")\n",
        "    test_df = pd.read_csv(splits_dir / \"test.csv\")\n",
        "\n",
        "    y_train = train_df[\"HiringDecision\"]\n",
        "    X_train = train_df.drop(columns=[\"HiringDecision\"])\n",
        "    y_test = test_df[\"HiringDecision\"]\n",
        "    X_test = test_df.drop(columns=[\"HiringDecision\"])\n",
        "\n",
        "    num_cols = [\n",
        "        \"Age\", \"ExperienceYears\", \"PreviousCompanies\", \"DistanceFromCompany\",\n",
        "        \"InterviewScore\", \"SkillScore\", \"PersonalityScore\"\n",
        "    ]\n",
        "    cat_cols = [\"Gender\", \"EducationLevel\", \"RecruitmentStrategy\"]\n",
        "\n",
        "    num_cols = [c for c in num_cols if c in X_train.columns]\n",
        "    cat_cols = [c for c in cat_cols if c in X_train.columns]\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, num_cols),\n",
        "            (\"cat\", categorical_transformer, cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", clf),\n",
        "    ])\n",
        "\n",
        "    # entrenamiento\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    # evaluaci√≥n\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1_pos = f1_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "    print(f\"[preprocess_and_train] Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"[preprocess_and_train] Test F1-score (clase 1 - contratado): {f1_pos:.4f}\")\n",
        "\n",
        "    # guardar modelo\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = models_dir / \"pipeline.joblib\"\n",
        "    joblib.dump(pipe, model_path)\n",
        "    print(f\"[preprocess_and_train] Modelo guardado en: {model_path}\")\n",
        "\n",
        "\n",
        "# Cuarta definicion: gradio_interface(**kwargs)--> UI m√≠nima para predecir desde JSON\n",
        "\n",
        "def predict(file, model_path):\n",
        "    \"\"\"\n",
        "    Carga el modelo y predice a partir de un archivo JSON con las features.\n",
        "    Soporta gr.File (con .name) o una ruta de archivo (str/Path).\n",
        "    \"\"\"\n",
        "    pipeline = joblib.load(model_path)\n",
        "    path = file.name if hasattr(file, \"name\") else file\n",
        "    input_data = pd.read_json(path)\n",
        "    preds = pipeline.predict(input_data)\n",
        "    labels = [\"No contratado\" if p == 0 else \"Contratado\" for p in preds]\n",
        "    return {\"Predicci√≥n\": labels[0]}\n",
        "\n",
        "\n",
        "def gradio_interface(**kwargs):\n",
        "    \"\"\"\n",
        "    Interfaz Gradio que carga el modelo desde <ds>/models/pipeline.joblib.\n",
        "    \"\"\"\n",
        "    import gradio as gr\n",
        "\n",
        "    run_folder_name = kwargs[\"ds\"]\n",
        "    dags_dir = Path(__file__).resolve().parent\n",
        "    model_path = dags_dir / run_folder_name / \"models\" / \"pipeline.joblib\"\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=lambda file: predict(file, model_path),\n",
        "        inputs=gr.File(label=\"Sube un archivo JSON\"),\n",
        "        outputs=\"json\",\n",
        "        title=\"Hiring Decision Prediction\",\n",
        "        description=(\n",
        "            \"Sube un archivo JSON con las caracter√≠sticas de entrada \"\n",
        "            \"para predecir si Vale ser√° contratada o no.\"\n",
        "        ),\n",
        "        allow_flagging=\"never\",\n",
        "    )\n",
        "\n",
        "    interface.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=True,    \n",
        "        inbrowser=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTKOj1hfodvm"
      },
      "source": [
        "## **1.2 Creando Nuestro DAG** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkEZcEh4odvm"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/a_yibuZQgngAAAAM/elle-woods.gif\" width=\"400\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-MTaxTgodvm"
      },
      "source": [
        "Con las funciones del pipeline ya creadas, ahora vamos a proceder a crear un Directed Acyclic Graph (DAG). Para ello, se le pide lo siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-yUak2Rodvm"
      },
      "source": [
        "- (10 puntos) Cree un segundo archivo llamado `dag_lineal.py` y guardelo en la carpeta dags. Este script debe seguir la siguiente estructura (Ver imagen de referencia):\n",
        "\n",
        "    0. Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, ejecuci√≥n manual y **sin backfill**. Asigne un `dag_id` que pueda reconocer facilmente, como `hiring_lineal`, etc.\n",
        "    1. Debe comenzar con un marcador de posici√≥n que indique el inicio del pipeline.\n",
        "    2. Cree una carpeta correspondiente a la ejecuci√≥n del pipeline y cree las subcarpetas `raw`, `splits` y `models` mediante la funci√≥n `create_folders()`.\n",
        "    3. Debe descargar el archivo `data_1.csv` del siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv). Debe guardar el archivo en la carpeta raw de la ejecuci√≥n correspondiente.`Hint:` Le puede ser √∫til el comando `curl -o <path de guardado> <enlace con los datos>`.\n",
        "    4. Debe aplicar un hold out mediante la funci√≥n `split_data()` de su archivo creado en la subsecci√≥n anterior.\n",
        "    5. Debe aplicar el preprocesamiento y el entrenamiento del modelo mediante la funci√≥n `preprocess_and_train()`.\n",
        "    6. Finalmente, debe montar una interfaz en gradio donde pueda cargar un archivo ``json``.\n",
        "\n",
        "\n",
        "- (3 puntos) Cree un `DockerFile` para montar un contenedor que contenga Airflow. Adicionalmente, cree una carpeta llamada dags donde guardar√° el script.py creado anteriormente.\n",
        "\n",
        "    `Nota:` Para la imagen, se recomienda utilizar python 3.10-slim. Adicionalmente, puede instalar `curl` mediante la siguiente linea de c√≥digo: `RUN apt-get update && apt-get install -y curl`.\n",
        "\n",
        "- Construya el contenedor en Docker y acceda a la aplicaci√≥n web de Airflow mediante el siguiente [enlace](http://localhost:8080/). Inicie sesi√≥n, acceda al DAG creado y ejecute de forma manual su pipeline.\n",
        "\n",
        "- (2 puntos) Acceda a la URL p√∫blica de Gradio e ingrese el archivo `vale_data.json` a su modelo. ¬øQue predicci√≥n entreg√≥ el modelo para Vale? Adjunte im√°genes de su resultado. `Hint:` Puede acceder a los `logs` para obtener los prints y la URL p√∫blica.\n",
        "\n",
        "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci√≥n `ds`.\n",
        "\n",
        "**Para esta secci√≥n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser√°n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im√°genes de apoyo, como screenshots.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiMTgQfJpuIv"
      },
      "source": [
        "DAG de referencia:\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1iwDgECZfFeWq1dl433tMa6_3CNF9cn1L\" width=\"1200\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckzDqsF4odvn"
      },
      "outputs": [],
      "source": [
        "#Inserte c√≥digo aqui\n",
        "\n",
        "# creo archivos con comando de powershel para vscode\n",
        "# Tengo qeu seguir en la carpeta del lab9\n",
        "cd Laboratorios\\Laboratorio_9\n",
        "\n",
        "creo el archivo\n",
        "New-Item -Path \".\\dags\\dag_lineal.py\" -ItemType \"File\" -Force\n",
        "\n",
        "## dag_lineal.py\n",
        "\n",
        "# Librer√≠as\n",
        "from datetime import datetime\n",
        "from airflow import DAG\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "from hiring_functions import (create_folders, split_data, preprocess_and_train, gradio_interface)\n",
        "\n",
        "# DAG con ejecuci√≥n manual, sin backfill, start 2024-10-01.\n",
        "\n",
        "with DAG(\n",
        "    dag_id=\"hiring_lineal\",\n",
        "    start_date=datetime(2024, 10, 1),\n",
        "    schedule=None,         \n",
        "    catchup=False,        # esto es lo del backfill\n",
        "    tags=[\"lab9\"],\n",
        "    is_paused_upon_creation=False,\n",
        ") as dag:\n",
        "\n",
        "# Marcador de posici√≥n que indica el inicio del pipeline.\n",
        "\n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "\n",
        "# Creaci√≥n carpetas  para el pipeline y subcarpetas raw, splits y models con la funci√≥n create_folders()\n",
        "\n",
        "    mk_run_folders = PythonOperator(\n",
        "        task_id=\"create_folders\",\n",
        "        python_callable=create_folders,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "    )\n",
        "\n",
        "# Descarga de data_1.csv para guardar en carpeta raw de la ejecuci√≥n\n",
        "\n",
        "    download_raw = BashOperator(\n",
        "    task_id=\"download_raw\",\n",
        "    bash_command=(\n",
        "        \"mkdir -p {{ dag.folder }}/{{ ds }}/raw && \"\n",
        "        \"curl -sSL -o {{ dag.folder }}/{{ ds }}/raw/data_1.csv \"\n",
        "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Aplico hold out con la funci√≥n split_data()\n",
        "\n",
        "    do_split = PythonOperator(\n",
        "        task_id=\"split_data\",\n",
        "        python_callable=split_data,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "    )\n",
        "\n",
        "# Aplico preprocesamiento y entrenamiento del modelo con la funci√≥n preprocess_and_train()\n",
        "\n",
        "    train_model = PythonOperator(\n",
        "        task_id=\"preprocess_and_train\",\n",
        "        python_callable=preprocess_and_train,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "    )\n",
        "\n",
        "# Interfaz de gradio para cargar archivo json\n",
        "\n",
        "    launch_gradio = PythonOperator(\n",
        "        task_id=\"gradio_interface\",\n",
        "        python_callable=gradio_interface,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "    )\n",
        "\n",
        "    start >> mk_run_folders >> download_raw >> do_split >> train_model >> launch_gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eset es mi c√≥digo del archivo Dockerfile.dockerfile (no va a correr aca obviamente al no ser .py)\n",
        "\n",
        "# Me redirecciono a mi carpte\n",
        "cd Laboratorios\\Laboratorio_9\n",
        "\n",
        "# Creaci√≥n del dockerfile\n",
        "New-Item -Path . -Name \"Dockerfile\" -ItemType \"File\"\n",
        "\n",
        "\n",
        "\n",
        "# Dockerfile.dockerfile\n",
        "# version de python recomendada en el enunciado\n",
        "FROM python:3.10-slim\n",
        "\n",
        "ENV DEBIAN_FRONTEND=noninteractive \\\n",
        "    PYTHONUNBUFFERED=1 \\\n",
        "    AIRFLOW_HOME=/opt/airflow\n",
        "\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "    curl build-essential git \\\n",
        " && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "RUN pip install --no-cache-dir \"apache-airflow==2.10.2\" \\\n",
        "  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.10.txt\"\n",
        "\n",
        "RUN pip install --no-cache-dir \\\n",
        "    pandas scikit-learn joblib numpy \\\n",
        "    gradio huggingface_hub\n",
        "\n",
        "RUN mkdir -p ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins\n",
        "WORKDIR ${AIRFLOW_HOME}\n",
        "COPY dags/ ./dags/\n",
        "\n",
        "EXPOSE 8080 7860\n",
        "CMD [\"bash\", \"-lc\", \"airflow standalone\"]\n",
        "\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## con este comando construyo la imagen desde Power Shell\n",
        "\n",
        "## CLAVE ABRIR/ENCENDER DOCKER DESKTOP O SIMILAR\n",
        "\n",
        "# Primero hay que estar parado en la carpeta, si no se esta ah√≠ use este comentario sin el #\n",
        "# cd Laboratorios\\Laboratorio_9\n",
        "\n",
        "# Tuve que especificar el nmbre de mi archivo pq por defecto los guarda con extension y docker no reconcoa eso me daba error\n",
        "\n",
        "# Build\n",
        "docker build -f Dockerfile.dockerfile -t airflow-hiring:latest .\n",
        "\n",
        "# Limpia contenedor anterior\n",
        "docker rm -f hiring-airflow 2>$null\n",
        "\n",
        "# Limpia PIDs\n",
        "Remove-Item -Force .\\airflow_home\\airflow-webserver.pid -ErrorAction SilentlyContinue\n",
        "Remove-Item -Force .\\airflow_home\\airflow-scheduler.pid -ErrorAction SilentlyContinue\n",
        "\n",
        "# Run: SOLO dag_lineal + sus funciones utilitarias\n",
        "docker run --name hiring-airflow `\n",
        "  -p 8080:8080 -p 7860:7860 `\n",
        "  -e AIRFLOW__CORE__LOAD_EXAMPLES=False `\n",
        "  -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor `\n",
        "  -e AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080 `\n",
        "  -v \"${PWD}\\airflow_home:/opt/airflow\" `\n",
        "  -v \"${PWD}\\dags\\dag_lineal.py:/opt/airflow/dags/dag_lineal.py\" `\n",
        "  -v \"${PWD}\\dags\\hiring_functions.py:/opt/airflow/dags/hiring_functions.py\" `\n",
        "  -v \"${PWD}\\plugins:/opt/airflow/plugins\" `\n",
        "  airflow-hiring:latest airflow standalone\n",
        "\n",
        "### luego de esto deberiamos poder acceder a este link:\n",
        "http://localhost:8080/ # Airflow imprime directaemte als credencialex admin/contrase√±a\n",
        "\n",
        "# Despu√©s tengo que esperar a que la parte de gradio quede como runn, abrir el log y ahi se ve la web publica que proporciona gradio y te permite ingresar la base de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El DAG `hiring_lineal.py` se ejecut√≥ correctamente, permitiendo exponer el modelo entrenado mediante una interfaz p√∫blica de **Gradio**, desde la cual se realizaron predicciones individuales y se verific√≥ el desempe√±o del modelo.\n",
        "\n",
        "**Flujo del DAG**\n",
        "\n",
        "![Flujo del DAG del hiring_lineal.py](DAG.png)\n",
        "\n",
        "**Log de `gradio_interface` ‚Äì URL p√∫blica del modelo**\n",
        "\n",
        "![Log de gradio_interface](log_gradio_interface.png)\n",
        "\n",
        "**Resultado de predicci√≥n para Vale**\n",
        "\n",
        "![Predicci√≥n modelo Vale](hiring_decision_prediction.png)\n",
        "\n",
        "**M√©tricas de desempe√±o (Accuracy y F1-Score)**\n",
        "\n",
        "![Accuracy y F1 score](log_y_valores_accu_f1.png)\n",
        "\n",
        "El modelo indica que con el archivo `vale_data.json`, Vale no ser√≠a contratada (0).  \n",
        "El accuracy del modelo en el conjunto de prueba fue de 0.9267, y el F1-score de la clase positiva (*contratada*) alcanz√≥ 0.8706, mostrando un buen equilibrio entre precisi√≥n y recall.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBlHcBQpXJb"
      },
      "source": [
        "# **2. Paralelizando el Pipeline** (30 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.gifer.com/8LNL.gif\" width=\"400\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoQaVOeiqO_R"
      },
      "source": [
        "Al ver los resultados obtenidos, Vale queda muy contenta con el clasificador. Sin embargo, le aparecen algunas dudas respecto al funcionamiento del pipeline. Primero le comenta que es posible que en un futuro tenga nuevos datos que podr√≠an ser √∫tiles para realizar nuevos entrenamientos, por lo que ser√≠a ideal si este pipeline se fuera ejecutando de forma peri√≥dica y **NO** de forma manual. Adem√°s, Vale le menciona que le gustar√≠a explorar el desempe√±o de otros modelos adem√°s de `Random Forest`, de forma que el pipeline seleccione de forma autom√°tica el modelo con mejor desempe√±o para luego hacer la predicci√≥n de Vale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mGPMg0ur-wR"
      },
      "source": [
        "## **2.1 Preparando un Nuevo Pipeline** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpU81VCRr-Hr"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/gnA7-5TewXMAAAAM/elle-woods.gif\" width=\"400\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KcXuS6bsZAw"
      },
      "source": [
        "De acuerdo a lo que le coment√≥ Vale, usted decide crear un nuevo script con las funciones que utilizar√° su pipeline. Por ende, dentro de la carpeta `dags`, usted crear√° el archivo `hiring_dynamic_functions.py` el cual debe contener:\n",
        "\n",
        "1. (2 puntos) Una funci√≥n llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecuci√≥n como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
        "  - raw\n",
        "  - preprocessed\n",
        "  - splits\n",
        "  - models\n",
        "2. (2 puntos) Una funci√≥n llamada `load_ands_merge()` que lea desde la carpeta `raw` los archivos `data_1.csv`y `data_2.csv` en caso de estar disponible. Luego concatene estos y genere un nuevo archivo resultante, guard√°ndolo en la carpeta `preprocessed`.\n",
        "\n",
        "3. (2 puntos) Una funci√≥n llamada `split_data()` que lea la data guardada en la carpeta `preprocessed` y realice un hold out sobre esta data. Esta funci√≥n debe crear un conjunto de entrenamiento y uno de prueba. Mantenga una semilla y 20% para el conjunto de prueba. Guarde los conjuntos resultantes en la carpeta `splits`.\n",
        "\n",
        "4. (6 puntos) Una funci√≥n llamada `train_model()` que reciba un modelo de clasificaci√≥n.\n",
        "    - La funci√≥n debe comenzar leyendo el conjunto de entrenamiento desde la carpeta `spits`.\n",
        "    - Esta debe crear y aplicar un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes.\n",
        "    - A√±ada una etapa de entrenamiento utilizando un modelo que ingrese a la funci√≥n.\n",
        "  \n",
        "  Esta funci√≥n **debe crear un archivo joblib con el pipeline entrenado**. Guarde el modelo con un nombre que le permita una facil identificaci√≥n dentro de la carpeta `models`.\n",
        "\n",
        "5. (3 puntos) Una funci√≥n llamada `evaluate_models()` que reciba sus modelos entrenados desde la carpeta `models`, eval√∫e su desempe√±o mediante `accuracy` en el conjunto de prueba y seleccione el mejor modelo obtenido. Luego guarde el mejor modelo como archivo `.joblib`. Su funci√≥n debe imprimir el nombre del modelo seleccionado y el accuracy obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnX61hxjW9rI"
      },
      "outputs": [],
      "source": [
        "#Inserte c√≥digo aqui\n",
        "\n",
        "# Creo un nuevo hyring_dynamics\n",
        "New-Item -Path \"dags/hiring_dynamic_functions.py\" -ItemType \"File\"\n",
        "\n",
        "\n",
        "# hiring_dynamic_functions.py\n",
        "\n",
        "#Librer√≠as\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Primera definici√≥n: create_folders(**context) --> Crea YYYY-MM-DD/(raw, preprocessed, splits, models)\n",
        "\n",
        "def create_folders(**context):\n",
        "    ds = context[\"ds\"]                               \n",
        "    dags_dir = Path(__file__).resolve().parent       \n",
        "    run_dir  = dags_dir / ds\n",
        "\n",
        "    for sub in [\"raw\", \"preprocessed\", \"splits\", \"models\"]:\n",
        "        (run_dir / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Segunda definici√≥n: load_and_merge(**context) --> Lee raw/data_1.csv (+ data_2.csv si existe), concat y guarda preprocessed/merged.csv\n",
        "\n",
        "def load_and_merge(**context):\n",
        "    ds = context[\"ds\"]\n",
        "    base = Path(__file__).resolve().parent / ds\n",
        "    raw_dir = base / \"raw\"\n",
        "    prep_dir = base / \"preprocessed\"\n",
        "    prep_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    p1 = raw_dir / \"data_1.csv\"\n",
        "    p2 = raw_dir / \"data_2.csv\"\n",
        "\n",
        "    dfs = []\n",
        "    if p1.exists():\n",
        "        dfs.append(pd.read_csv(p1))\n",
        "        print(f\"[load_and_merge] Le√≠do: {p1}\")\n",
        "    if p2.exists():\n",
        "        dfs.append(pd.read_csv(p2))\n",
        "        print(f\"[load_and_merge] Le√≠do: {p2}\")\n",
        "\n",
        "    merged = pd.concat(dfs, ignore_index=True)\n",
        "    out = prep_dir / \"merged.csv\"\n",
        "    merged.to_csv(out, index=False)\n",
        "    print(f\"[load_and_merge] Guardado: {out} (n={len(merged)})\")\n",
        "\n",
        "\n",
        "# Tercera definici√≥n: split_data(**context) --> Lee preprocessed/merged.csv y hace hold-out 80/20 (seed=42)\n",
        "\n",
        "def split_data(**context):\n",
        "    ds = context[\"ds\"]\n",
        "    base = Path(__file__).resolve().parent / ds\n",
        "    merged_path = base / \"preprocessed\" / \"merged.csv\"\n",
        "\n",
        "    df = pd.read_csv(merged_path)\n",
        "\n",
        "    y = df[\"HiringDecision\"]\n",
        "    X = df.drop(columns=[\"HiringDecision\"])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    splits_dir = base / \"splits\"\n",
        "    splits_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (X_train.assign(HiringDecision=y_train)).to_csv(splits_dir / \"train.csv\", index=False)\n",
        "    (X_test.assign(HiringDecision=y_test)).to_csv(splits_dir / \"test.csv\", index=False)\n",
        "\n",
        "\n",
        "# Cuarta definici√≥n: train_model(model, model_name=None, **context) --> Entrena pipeline(preproc + modelo) y guarda models/<nombre>.joblib\n",
        "# Ac√° tambi√©n defino el preprocessor de las bases .csv\n",
        "\n",
        "def _build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Devuelve un ColumnTransformer minimalista (compatible con sklearn 1.3.x).\n",
        "    \"\"\"\n",
        "    num_cols = [\n",
        "        \"Age\", \"ExperienceYears\", \"PreviousCompanies\", \"DistanceFromCompany\",\n",
        "        \"InterviewScore\", \"SkillScore\", \"PersonalityScore\",\n",
        "    ]\n",
        "    cat_cols = [\"Gender\", \"EducationLevel\", \"RecruitmentStrategy\"]\n",
        "\n",
        "    num_cols = [c for c in num_cols if c in X.columns]\n",
        "    cat_cols = [c for c in cat_cols if c in X.columns]\n",
        "\n",
        "    numeric = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "\n",
        "    categorical = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric, num_cols),\n",
        "            (\"cat\", categorical, cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "\n",
        "def train_model(model, model_name: str | None = None, **context):\n",
        "    \"\"\"\n",
        "    Entrena (preprocesamiento + modelo) sobre <ds>/splits/train.csv\n",
        "    y guarda el pipeline en <ds>/models/<nombre>.joblib\n",
        "    - model: estimador sklearn ya instanciado (clasificador)\n",
        "    - model_name: nombre del archivo .joblib (opcional). Si no, usa la clase del modelo.\n",
        "    \"\"\"\n",
        "    ds = context[\"ds\"]\n",
        "    base = Path(__file__).resolve().parent / ds\n",
        "    splits_dir = base / \"splits\"\n",
        "    models_dir = base / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_path = splits_dir / \"train.csv\"\n",
        "\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    target_col = \"HiringDecision\"\n",
        "\n",
        "    y_train = train_df[target_col]\n",
        "    X_train = train_df.drop(columns=[target_col])\n",
        "\n",
        "    preprocessor = _build_preprocessor(X_train)\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    base_name = model_name or model.__class__.__name__\n",
        "    out_path = models_dir / f\"{base_name}.joblib\"\n",
        "    joblib.dump(pipe, out_path)\n",
        "    print(f\"[train_model] Modelo entrenado y guardado en: {out_path}\")\n",
        "\n",
        "\n",
        "# Quinta definici√≥n: evaluate_models(**context) --> Eval√∫a todos los .joblib en models/ con accuracy en test y selecciona el mejor y lo guarda como models/best_model.joblib\n",
        "\n",
        "def evaluate_models(**context):\n",
        "    ds = context[\"ds\"]\n",
        "    base = Path(__file__).resolve().parent / ds\n",
        "    splits_dir = base / \"splits\"\n",
        "    models_dir = base / \"models\"\n",
        "\n",
        "    test_path = splits_dir / \"test.csv\"\n",
        "\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    y_test = test_df[\"HiringDecision\"]\n",
        "    X_test = test_df.drop(columns=[\"HiringDecision\"])\n",
        "\n",
        "    candidates = sorted([p for p in models_dir.glob(\"*.joblib\") if p.name != \"best_model.joblib\"])\n",
        "\n",
        "    best_name = None\n",
        "    best_score = -1.0\n",
        "    best_pipe = None\n",
        "\n",
        "    for path in candidates:\n",
        "        pipe = joblib.load(path)\n",
        "        y_pred = pipe.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"[evaluate_models] {path.name}: accuracy={acc:.4f}\")\n",
        "        if acc > best_score:\n",
        "            best_score = acc\n",
        "            best_name = path.name\n",
        "            best_pipe = pipe\n",
        "\n",
        "    out_best = models_dir / \"best_model.joblib\"\n",
        "    joblib.dump(best_pipe, out_best)\n",
        "    print(f\"[evaluate_models] Mejor modelo: {best_name} | accuracy={best_score:.4f}\")\n",
        "    print(f\"[evaluate_models] Guardado como: {out_best}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUYkXWcZJz3b"
      },
      "source": [
        "## **2.2 Componiendo un nuevo DAG** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak7uL9YXJ6Xj"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://67.media.tumblr.com/bfa5208006dc3f404ec08e8c3195cf2c/tumblr_obg9tgnLfX1u9e9f2o2_r1_500.gif\" width=\"500\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbE6mu20LfWd"
      },
      "source": [
        "Con las nuevas funciones, se debe crear el nuevo nuevo DAG. Para ello, cree un nuevo script en la carpeta `dags`, llamandolo `dag_dynamic.py`. Este script debe contener la siguiente estructura:\n",
        "\n",
        "1. (1 punto) Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, el cual se debe ejecutar el d√≠a 5 de cada mes a las 15:00 UTC. Utilice un `dag_id` interpretable para identificar f√°cilmente. **Habilite el backfill** para que pueda ejecutar tareas programadas desde fechas pasadas.\n",
        "2. (1 punto) Comience con un marcador de posici√≥n que indique el inicio del pipeline.\n",
        "3. (2 puntos) Cree una carpeta correspondiente a la ejecuci√≥n del pipeline y cree las subcarpetas `raw`, `preprocessed`, `splits` y `models` mediante la funci√≥n `create_folders()`.\n",
        "4. (2 puntos) Implemente un `Branching`que siga la siguiente l√≥gica:\n",
        "  - Fechas previas al 1 de noviembre de 2024: Se descarga solo `data_1.csv`\n",
        "  - Desde el 1 de noviembre del 2024: descarga `data_1.csv` y `data_2.csv`.\n",
        "  En el siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv) puede descargar `data_2.csv`.\n",
        "5. (1 punto) Cree una tarea que concatene los datasets disponibles mediante la funci√≥n `load_and_merge()`. Configure un `Trigger` para que la tarea se ejecute si encuentra disponible **como m√≠nimo** uno de los archivos.\n",
        "6. (1 punto) Aplique el hold out al dataset mediante la funci√≥n `split_data()`, obteniendo un conjunto de entrenamiento y uno de prueba.\n",
        "7. (2 puntos) Realice 3 entrenamientos en paralelo:\n",
        "  - Un modelo Random Forest.\n",
        "  - 2 modelos a elecci√≥n.\n",
        "  Aseg√∫rese de guardar sus modelos entrenados con nombres distintivos. Utilice su funci√≥n `train_model()` para ello.\n",
        "8. (2 puntos) Mediante la funci√≥n `evaluate_models()`, eval√∫e los modelos entrenados, registrando el accuracy de cada modelo en el set de prueba. Luego debe imprimir el mejor modelo seleccionado y su respectiva m√©trica. Configure un `Trigger` para que la tarea se ejecute solamente si los 3 modelos fueron entrenados y guardados.\n",
        "\n",
        "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecuci√≥n `ds`.\n",
        "\n",
        "Una vez creado el script, vuelva a construir el contenedor en Docker, acceda a la aplicaci√≥n web de Airflow, ejecute su pipeline y muestre sus resultados. Adjunte im√°genes que ayuden a mostrar el proceso y sus resultados.\n",
        "\n",
        "Adicionalmente, responda (1 c/u):\n",
        "\n",
        "- ¬øCual es el accuracy de cada modelo en la ejecuci√≥n de octubre? ¬øSe obtienen los mismos resultados a partir de Noviembre?\n",
        "- Analice como afect√≥ el a√±adir datos a sus modelos mediante el desempe√±o del modelo y en costo computacional.\n",
        "- Muestre el esquema de su DAG ejecutado en octubre y en noviembre.\n",
        "\n",
        "\n",
        "`Nota:` Para esta secci√≥n no debe implementar la tarea en gradio, solamente se espera determinar el mejor modelo y comparar el desempe√±o obtenido.\n",
        "\n",
        "**IMPORTANTE: Para esta secci√≥n, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que ser√°n ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar im√°genes de apoyo, como screenshots.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMgK2sKTYJji"
      },
      "outputs": [],
      "source": [
        "#Inserte c√≥digo aqui\n",
        "\n",
        "# Creo un nuevo file dag_dynamics\n",
        "New-Item -Path \"dags/dag_dynamic.py\" -ItemType \"File\"\n",
        "\n",
        "# dag_dynamic.py\n",
        "\n",
        "# Librer√≠as\n",
        "from datetime import datetime\n",
        "import pendulum\n",
        "from airflow import DAG\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
        "from airflow.utils.trigger_rule import TriggerRule\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from hiring_dynamic_functions import (create_folders, load_and_merge, split_data, train_model, evaluate_models)\n",
        "\n",
        "\n",
        "# DAG --> corre el d√≠a 5 de cada mes a las 15:00 UTC: backfill HABILITADO (catchup=True) y start_date = 2024-10-01\n",
        "\n",
        "with DAG(\n",
        "    dag_id=\"hiring_dynamic\",               \n",
        "    start_date=datetime(2024, 10, 1),\n",
        "    schedule=\"0 15 5 * *\",                   # 5 de cada mes, 15:00 UTC\n",
        "    catchup=True,                            # habilita backfill\n",
        "    tags=[\"lab9\", \"airflow\", \"dynamic\"],\n",
        "    is_paused_upon_creation=False,\n",
        ") as dag:\n",
        "\n",
        "# Marcador de inicio\n",
        "    start = EmptyOperator(task_id=\"start\")\n",
        "\n",
        "# Carpetas YYYY-MM-DD/{raw, preprocessed, splits, models}\n",
        "    t_create = PythonOperator(\n",
        "        task_id=\"create_folders\",\n",
        "        python_callable=create_folders,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "    )\n",
        "\n",
        "# Branching por fecha de ejecuci√≥n, antes del 2024-11-01: solo data_1.csv y desde 2024-11-01 inclusive: data_1.csv y data_2.csv\n",
        "def branching(**context):\n",
        "    logical_date = context[\"logical_date\"].in_timezone(\"UTC\")\n",
        "    cutoff = pendulum.datetime(2024, 11, 1, tz=\"UTC\")\n",
        "    if logical_date < cutoff:\n",
        "        return \"download_data1\"\n",
        "\n",
        "    return [\"download_data1\", \"download_data2\"]\n",
        "\n",
        "branch = BranchPythonOperator(\n",
        "    task_id=\"branch_by_date\",\n",
        "    python_callable=branching,\n",
        ")\n",
        "\n",
        "# Descargo de als baases de datos seg√∫n branching\n",
        "t_dl1 = BashOperator(\n",
        "    task_id=\"download_data1\",\n",
        "    bash_command=(\n",
        "        \"mkdir -p {{ dag.folder }}/{{ ds }}/raw && \"\n",
        "        \"curl -sSL -o {{ dag.folder }}/{{ ds }}/raw/data_1.csv \"\n",
        "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "t_dl2 = BashOperator(\n",
        "    task_id=\"download_data2\",\n",
        "    bash_command=(\n",
        "        \"mkdir -p {{ dag.folder }}/{{ ds }}/raw && \"\n",
        "        \"curl -sSL -o {{ dag.folder }}/{{ ds }}/raw/data_2.csv \"\n",
        "        \"https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Merge de datasets disponibles, debe correr si hay al menos UNO disponible ‚Üí TriggerRule.ONE_SUCCESS\n",
        "t_merge = PythonOperator(\n",
        "        task_id=\"load_and_merge\",\n",
        "        python_callable=load_and_merge,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "        trigger_rule=TriggerRule.ONE_SUCCESS,\n",
        "    )\n",
        "\n",
        "# split hold-out (80/20 con semilla) ‚Üí splits/train.csv y splits/test.csv\n",
        "t_split = PythonOperator(\n",
        "        task_id=\"split_data\",\n",
        "        python_callable=split_data,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "    )\n",
        "\n",
        "# Tres entrenamientos en paralelo\n",
        "t_train_rf = PythonOperator(\n",
        "        task_id=\"train_rf\",\n",
        "        python_callable=train_model,\n",
        "        op_kwargs={\n",
        "            \"ds\": \"{{ ds }}\",\n",
        "            \"model\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
        "            \"model_name\": \"RandomForest\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "t_train_lr = PythonOperator(\n",
        "        task_id=\"train_lr\",\n",
        "        python_callable=train_model,\n",
        "        op_kwargs={\n",
        "            \"ds\": \"{{ ds }}\",\n",
        "            \"model\": LogisticRegression(max_iter=1000),\n",
        "            \"model_name\": \"LogReg\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "t_train_dt = PythonOperator(\n",
        "        task_id=\"train_dt\",\n",
        "        python_callable=train_model,\n",
        "        op_kwargs={\n",
        "            \"ds\": \"{{ ds }}\",\n",
        "            \"model\": DecisionTreeClassifier(random_state=42),\n",
        "            \"model_name\": \"DecisionTree\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "# Evaluaci√≥n del mejor modelo, aca se va a correr solo si los 3 entrenamientos terminaron OK ‚Üí ALL_SUCCESS si no, no porque falla\n",
        "t_eval = PythonOperator(\n",
        "        task_id=\"evaluate_models\",\n",
        "        python_callable=evaluate_models,\n",
        "        op_kwargs={\"ds\": \"{{ ds }}\"},\n",
        "        trigger_rule=TriggerRule.ALL_SUCCESS,\n",
        "    )\n",
        "\n",
        "end = EmptyOperator(task_id=\"end\")\n",
        "\n",
        "# Estructura del DAG\n",
        "start >> t_create >> branch\n",
        "branch >> t_dl1\n",
        "branch >> t_dl2\n",
        "[t_dl1, t_dl2] >> t_merge >> t_split >> [t_train_rf, t_train_lr, t_train_dt] >> t_eval >> end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Vuelvo a crear un nuevo dockerfile.dockerfile\n",
        "# Eset es mi c√≥digo del archivo Dockerfile_dynamics.dockerfile (no va a correr aca obviamente al no ser .py)\n",
        "\n",
        "# Me redirecciono a mi carpte\n",
        "cd Laboratorios\\Laboratorio_9\n",
        "\n",
        "# Creaci√≥n del dockerfile\n",
        "New-Item -Path . -Name \"Dockerfile_dynamics\" -ItemType \"File\"\n",
        "\n",
        "\n",
        "# Dockerfile_dynamics\n",
        "FROM python:3.10-slim\n",
        "\n",
        "ENV DEBIAN_FRONTEND=noninteractive \\\n",
        "    PYTHONUNBUFFERED=1 \\\n",
        "    AIRFLOW_HOME=/opt/airflow\n",
        "\n",
        "# Requisitos de SO (incluye curl como pide el enunciado)\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "    curl build-essential git && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Airflow con constraints para Py3.10\n",
        "RUN pip install --no-cache-dir \"apache-airflow==2.10.2\" \\\n",
        "  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.10.txt\"\n",
        "\n",
        "# Paquetes de tu pipeline (pin de sklearn para compat con sparse_output=False)\n",
        "RUN pip install --no-cache-dir \\\n",
        "    pandas==2.2.2 scikit-learn==1.3.2 joblib numpy \\\n",
        "    gradio\n",
        "\n",
        "# Estructura est√°ndar de Airflow\n",
        "RUN mkdir -p ${AIRFLOW_HOME}/dags ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/plugins\n",
        "WORKDIR ${AIRFLOW_HOME}\n",
        "\n",
        "# (Opcional) Copiar dags en build; normalmente los montaremos por volumen al correr\n",
        "# COPY dags/ ./dags/\n",
        "\n",
        "EXPOSE 8080 7860\n",
        "CMD [\"bash\", \"-lc\", \"airflow standalone\"]\n",
        "\n",
        "\n",
        "##-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## CLAVE ABRIR/ENCENDER DOCKER DESKTOP O SIMILAR\n",
        "\n",
        "# Primero hay que estar parado en la carpeta, si no se esta ah√≠ use este comentario sin el #\n",
        "# cd Laboratorios\\Laboratorio_9\n",
        "\n",
        "# Tuve que especificar el nmbre de mi archivo pq por defecto los guarda con extension y docker no reconcoa eso me daba error\n",
        "\n",
        "# Build imagen din√°mica\n",
        "docker build -f Dockerfile_dynamics -t airflow-hiring:latest .\n",
        "\n",
        "# Elimina contenedor previo\n",
        "docker rm -f hiring-airflow 2>$null\n",
        "\n",
        "# Crea y levanta Airflow\n",
        "docker run --name hiring-airflow `\n",
        "  -p 8080:8080 `\n",
        "  -e AIRFLOW__CORE__LOAD_EXAMPLES=False `\n",
        "  -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor `\n",
        "  -e AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080 `\n",
        "  -v \"$(pwd)/dags:/opt/airflow/dags\" `\n",
        "  -v \"$(pwd)/logs:/opt/airflow/logs\" `\n",
        "  -v \"$(pwd)/plugins:/opt/airflow/plugins\" `\n",
        "  airflow-hiring:latest\n",
        "\n",
        "# Web para revisar\n",
        "http://localhost:8080\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preguntas adicionales\n",
        "\n",
        "#### 1. Muestre el esquema de su DAG ejecutado en octubre y en noviembre\n",
        "\n",
        "**Flujo del DAG del hiring_dynamic.py --> octubre y noviemrbe**\n",
        "\n",
        "**OCTUBRE**\n",
        "\n",
        "![Flujo DAG octubre](DAGoct.png)\n",
        "![Diagrama octubre](diag_oct_2024.png)\n",
        "\n",
        "**NOVIEMBRE**\n",
        "\n",
        "![Flujo DAG noviembre](DAGnov.png)\n",
        "![Diagrama noviembre](diag_nov_2024.png)\n",
        "\n",
        "**Comprobaci√≥n del flujo global**\n",
        "![Flujo global correcto](comprobacion_global_que_funciona.png)\n",
        "\n",
        "#### 2. ¬øCual es el accuracy de cada modelo en la ejecuci√≥n de octubre? ¬øSe obtienen los mismos resultados a partir de Noviembre?\n",
        "\n",
        "**OCTUBRE**\n",
        "\n",
        "![M√©tricas mejor modelo octubre](metricas_mejor_modelo_oct.png)\n",
        "\n",
        "**NOVIEMBRE**\n",
        "\n",
        "![M√©tricas mejor modelo noviembre](metricas_mejor_modelo_nov.png)\n",
        "\n",
        "**Respuesta:** En la ejecuci√≥n de octubre (solo `data_1.csv`), los accuracy fueron:\n",
        "\n",
        "* DecisionTree: **0.8667**\n",
        "* LogReg: **0.9133**\n",
        "* RandomForest: **0.9267** ‚Üí mejor modelo.\n",
        "\n",
        "En noviembre (con `data_1.csv` + `data_2.csv`), los resultados fueron:\n",
        "\n",
        "* DecisionTree: **0.8800**\n",
        "* LogReg: **0.9100**\n",
        "* RandomForest: **0.9267** ‚Üí nuevamente el mejor.\n",
        "\n",
        "Los desempe√±os se mantuvieron pr√°cticamente iguales, con una leve mejora en los modelos m√°s simples y sin variaci√≥n en el Random Forest, que sigui√≥ siendo el m√°s preciso.\n",
        "\n",
        "Adem√°s, aunque no se muestran los logs de `evaluate_models` para los dem√°s meses, las ejecuciones posteriores confirman que a partir de noviembre los valores se estabilizan, ya que el dataset no cambia.  \n",
        "Si en el futuro se incorporaran nuevos datos, podr√≠an observarse variaciones menores, pero con la evidencia actual el desempe√±o de los modelos se mantiene constante.\n",
        "\n",
        "### 3. Analice c√≥mo afect√≥ el a√±adir datos a sus modelos mediante el desempe√±o del modelo y en costo computacional\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Al incorporar el archivo `data_2.csv` a partir de noviembre, el pipeline `hiring_dynamic.py` proces√≥ un mayor volumen de datos sin afectar el rendimiento general del modelo.  \n",
        "En ambas ejecuciones (octubre y noviembre), el **Random Forest** se mantuvo como el mejor clasificador con un *accuracy* estable de **0.9267**, seguido por Logistic Regression (~0.91) y Decision Tree (~0.88).  \n",
        "Esto demuestra que el modelo es **robusto y consistente frente al aumento de datos**, sin p√©rdida de precisi√≥n ni sobreajuste.\n",
        "\n",
        "En t√©rminos de **costo computacional**, el impacto fue m√≠nimo.  \n",
        "El tiempo total del DAG aument√≥ levemente ‚Äîde **00:04:15** en octubre a **00:05:13** en noviembre‚Äî, lo cual era esperable ya que en noviembre se ejecuta tambi√©n la descarga de `data_2.csv` y su fusi√≥n en la etapa `load_and_merge`.  \n",
        "El resto de las tareas (entrenamiento, evaluaci√≥n y splits) se mantuvo pr√°cticamente igual, confirmando que el pipeline escala bien y mantiene buena eficiencia incluso con m√°s registros.\n",
        "\n",
        "Adem√°s, al habilitar el **backfill (`catchup=True`)** en el DAG din√°mico, Airflow ejecut√≥ autom√°ticamente **todas las fechas pasadas programadas** desde el `start_date` definido (1 de octubre de 2024) hasta la fecha actual, corriendo las queries hist√≥ricas una por una.  \n",
        "Esto permiti√≥ observar c√≥mo el DAG se comporta mes a mes y c√≥mo gestiona el hist√≥rico completo, lo que es clave para flujos que se mantienen activos en producci√≥n y procesan datos acumulativos.  \n",
        "El backfill no solo valida la consistencia del pipeline en fechas pasadas, sino que tambi√©n asegura que **todas las corridas pendientes se ejecuten sin intervenci√≥n manual**, lo cual es exactamente el prop√≥sito de un flujo din√°mico.\n",
        "\n",
        "Las siguientes figuras muestran los tiempos promedio por tarea en las ejecuciones de octubre y noviembre:\n",
        "\n",
        "#### Creaci√≥n y branching inicial\n",
        "![Duraci√≥n create_folders](costo_computacional_create_folders.png)\n",
        "![Duraci√≥n branch_by_date](costo_computacional_branch.png)\n",
        "\n",
        "#### Descarga y preparaci√≥n de datos\n",
        "![Duraci√≥n download_data1](costo_computacional_data1.png)\n",
        "![Duraci√≥n download_data2](costo_computacional_data2.png)\n",
        "![Duraci√≥n load_and_merge](costo_computacional_loadmerge.png)\n",
        "![Duraci√≥n split_data](costo_computacional_splitdata.png)\n",
        "\n",
        "#### Entrenamiento de modelos\n",
        "![Duraci√≥n train_rf](costo_computacional_rf.png)\n",
        "![Duraci√≥n train_lr](costo_computacional_lr.png)\n",
        "![Duraci√≥n train_dt](costo_computacional_dt.png)\n",
        "\n",
        "#### Evaluaci√≥n final\n",
        "![Duraci√≥n evaluate_models](costo_computacional_eval_modelos.png)\n",
        "\n",
        "En conjunto, los resultados globales de ejecuci√≥n fueron los siguientes:\n",
        "\n",
        "| Fecha de ejecuci√≥n | Duraci√≥n total del DAG | Estado final |\n",
        "|--------------------|------------------------|---------------|\n",
        "| 2024-10-05 | **00:04:15** | `success` |\n",
        "| 2024-11-05 | **00:05:13** | `success` |\n",
        "\n",
        "El aumento de datos gener√≥ un incremento de alrededor de **1 minuto** en la duraci√≥n total del pipeline, atribuible casi por completo a la etapa de descarga y concatenaci√≥n de archivos, mientras que los tiempos de entrenamiento (entre **30 y 55 s**) se mantuvieron estables entre corridas.  \n",
        "\n",
        "Esto confirma que el pipeline **es eficiente, reproducible y escalable**, manteniendo el desempe√±o del modelo y el comportamiento temporal controlado aun cuando crece el volumen de datos.\n",
        "\n",
        "**OCTUBRE**\n",
        "\n",
        "![Duraci√≥n total del DAG ‚Äì octubre](diag_oct_2024.png)\n",
        "\n",
        "**NOVIEMBRE**\n",
        "\n",
        "![Duraci√≥n total del DAG ‚Äì noviembre](diag_nov_2024.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrmM65RIRrgm"
      },
      "source": [
        "# Conclusi√≥n\n",
        "\n",
        "√âxito!\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:1000/1*PX8WVijZapo7EDrvGv9Inw.gif\" width=\"500\">\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.13.5 ('vis_info')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "27f7193e8435d5833318a8779fcc7c01e1e51279cdc9a1fc598f78d31f0d2dc3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
