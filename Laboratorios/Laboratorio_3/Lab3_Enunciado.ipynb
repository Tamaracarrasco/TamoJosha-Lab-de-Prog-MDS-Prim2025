{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ATENCI√ìN - NO SE VEN LOS GR√ÅFICOS PUESTO A QUE PLOTLY NO LOS GUARDA PERMANENTEMENTE EN EL DOCUMENTO Y HACE CORRER EL C√ìDIGO CADA VEZ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**POR ELLO, LOS GR√ÅFICOS NO ESTAN VISIBLES PERO SE PUEDEN CORRER SIN PROBLEMA Y DARAN LOS OUTPUTS QUE SE RELACIONAN CON LOS AN√ÅLISIS QUE HICIMOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ADEM√ÅS, PARA EFECTOS DEL REPOSITORIO, OCURRIO LO MISMO, NO SE VEN OUTPUTS PORQUE PLOTLY HACIA QUE EL DOCUMENTO PESARA MAS DE LO QUE GITHUB SOPORTA, PERO EN T√âRMINOS DE C√ìDIGO ESTA TODO LO NECESARIO PARA CORRER NUESTRO LAB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8dd9b572c805487a9fb430fdc4ab12bb",
        "deepnote_cell_height": 156.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "XUZ1dFPHzAHl"
      },
      "source": [
        "<h1><center>Laboratorio 3: La desperaci√≥n de Mr. Cheems üêº</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d65413cd8566460dbceffcd13ca236e7",
        "deepnote_cell_type": "markdown",
        "id": "UD8X1uhGzAHq"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8e9217d02d124830a9b86046600a1605",
        "deepnote_cell_height": 172.13333129882812,
        "deepnote_cell_type": "markdown",
        "id": "tXflExjqzAHr"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Josefa Anselmo\n",
        "- Nombre de alumno 2: Tamara Carrasco\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "010402b6d5f743b885a80d2e1c6ae11a",
        "deepnote_cell_height": 62.19999694824219,
        "deepnote_cell_type": "markdown",
        "id": "AD-V0bbZzAHr"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [TamoJosha](https://github.com/Tamaracarrasco/TamoJosha-Lab-de-Prog-MDS-Prim2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ef0224c7a99e4b718b55493b0a1e99c4",
        "deepnote_cell_height": 724.9000244140625,
        "deepnote_cell_type": "markdown",
        "id": "6uBLPj1PzAHs"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar procesos m√°s limpios en Feature Engineering.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "59664481c26f4ac4a753765269b1db6a",
        "deepnote_cell_height": 69.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "wrG4gYabzAHs"
      },
      "source": [
        "## Descripci√≥n del laboratorio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8c7bf8ea553d44c7a2efd61106a0bac2",
        "deepnote_cell_height": 61.866668701171875,
        "deepnote_cell_type": "markdown",
        "id": "MhISwri4zAHy"
      },
      "source": [
        "### Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-29T00:08:16.884674Z",
          "start_time": "2021-03-29T00:08:16.349846Z"
        },
        "cell_id": "67b4b29f0e6b48719b58d579276f2b19",
        "deepnote_cell_height": 514.13330078125,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 8517,
        "execution_start": 1635469788590,
        "id": "uyc33dKdzAHy",
        "source_hash": "a3741fd5"
      },
      "outputs": [],
      "source": [
        "# Libreria Core del lab.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Libreria para plotear (En colab esta desactualizado plotly)\n",
        "%pip install --upgrade plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Librerias utiles\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ce6a19ec6fc6486e832760ac3740d7ef",
        "deepnote_cell_height": 219.46665954589844,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 7,
        "execution_start": 1635165625274,
        "id": "gQ0-zPV4NNrq",
        "outputId": "a7c33afa-37fe-4965-de1a-53b8994c8c07",
        "source_hash": "c60dc4a7"
      },
      "outputs": [],
      "source": [
        "# Si usted est√° utilizando Colabolatory le puede ser √∫til este c√≥digo para cargar los archivos.\n",
        "# try:\n",
        "  #  from google.colab import drive\n",
        "   # drive.mount(\"/content/drive\")\n",
        "   # path = 'Direcci√≥n donde tiene los archivos en el Drive'\n",
        "#except:\n",
        " #   print('Ignorando conexi√≥n drive-colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "28c7a8b483d84878ac5a4f7ba882b711",
        "deepnote_cell_height": 133.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "QDwIXTh7bK_A",
        "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb"
      },
      "source": [
        "# Feature engineering en datos de retail üõçÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "160bb2695f6547448bfb0f99420f952c",
        "deepnote_cell_height": 69.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "_Eu4qBqnXMff",
        "tags": []
      },
      "source": [
        "### 0. Cargar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6c6799ecc9e74272922d46a3b5a8b79e",
        "deepnote_cell_height": 294.683349609375,
        "deepnote_cell_type": "markdown",
        "id": "4shIzqqwXMfe",
        "tags": []
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://s1.eestatic.com/2018/04/14/social/la_jungla_-_social_299733421_73842361_854x640.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "48d29c89e3b6455083f8fac764f97f3b",
        "deepnote_cell_height": 475.066650390625,
        "deepnote_cell_type": "markdown",
        "id": "cDpKjYRCXMfg",
        "tags": []
      },
      "source": [
        "Mr. Cheems, gerente de una cotizada tienda de retail en Europa, les solicita si pueden analizar los datos de algunas de sus tiendas. En una reuni√≥n, Mr Cheems le comenta que la calidad de sus datos no es muy buena, por lo que le solicita a usted que limpie su base de datos y cree nuevos atributos relevantes para el negocio.\n",
        "\n",
        "Por ello, el √°rea de ventas les entrega archivo llamado `online_retail_data.pickle` el cual usted decide cargar a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.io as pio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "4d7d0f0855744e6c9d5a2198e5dcd690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "deepnote_cell_height": 489.79998779296875,
        "deepnote_cell_type": "code",
        "deepnote_output_heights": [
          177
        ],
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 466,
        "execution_start": 1635469797118,
        "id": "7FNOu-CvjV5m",
        "outputId": "90b4f92c-71df-44d4-8084-4dd06a6179e4",
        "source_hash": "d52b246c"
      },
      "outputs": [],
      "source": [
        "# Inserte su c√≥digo aqu√≠\n",
        "df_retail = pd.read_pickle(\"online_retail_data.pickle\")\n",
        "\n",
        "df_retail.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_retail[\"Price\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_retail[\"Quantity\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_retail.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6nm_0uWvrFv"
      },
      "source": [
        "### 1. Funci√≥n para explorar caracter√≠sticas [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZEZbbLoqfI"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://editor.analyticsvidhya.com/uploads/47389meme.png\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-7ZaNutk2GO"
      },
      "source": [
        "\n",
        "\n",
        "Tras inspeccionar brevemente los datos proporcionados, usted decide crear una funci√≥n que realice lo siguiente:\n",
        "- Plotee un histograma para las variables precios y cantidad. [0.3 puntos]\n",
        "- Imprima un conteo de datos nulos por variable [0.2 puntos]\n",
        "\n",
        "**Nota**: Para generar los gr√°ficos no es obligatorio el uso de `plotly`, pero si es altamente recomendado. Pueden encontrar m√°s informaci√≥n de esta librer√≠a en este [enlace](https://plotly.com/python/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM8FZ_4Yuiwi"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_data(df, name):\n",
        "    \"\"\"\n",
        "    Funci√≥n que retorna histogramas de las variables Price y Quantity de un dataframe que\n",
        "    contiene esas columnas.\n",
        "\n",
        "    Par√°metros:\n",
        "      df: DataFrame con columnas 'Price' y 'Quantity'\n",
        "      name: nombre del dataframe. Debe ser un string\n",
        "      yscale: modificamos la escala en el eje Y pues no se pod√≠a graficar \"bien\" con plotly.\n",
        "\n",
        "    \"\"\"\n",
        "    # Se agrega esto porque tuve problemas en el renderizado de vscode\n",
        "    pio.renderers.default = \"notebook\"\n",
        "    \n",
        "    # Imprimimos valores nulos\n",
        "    print(\"Valores nulos por columna\")\n",
        "    print(df.isna().sum())\n",
        "\n",
        "    # Histogramas con Plotly    \n",
        "\n",
        "    fig1 = px.histogram(\n",
        "        df, x=\"Price\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Price del dataframe {name}\",\n",
        "    ) # Aqu√≠ defino el plot para Prie\n",
        "\n",
        "    fig1.show()\n",
        "\n",
        "    fig2 = px.histogram(\n",
        "        df, x=\"Quantity\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Quantity del dataframe {name}\",\n",
        "    ) # Aqu√≠ defino el plot para Quantity\n",
        "  \n",
        "    fig2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# histogramas sin el suavizado logar√≠tmico\n",
        "\n",
        "explore_data(df_retail, \"df_retail\")\n",
        "\n",
        "# Como se puede ver, es muy dif√≠cil identificar el comportamiento de esta variable con esta escala\n",
        "# as√≠ que vamos a aplicar el log en la siguiente celda logrando as√≠ ver el comportamiento, en otra\n",
        "# escala, pero de este modo si funciona en el fondo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZY_N0Ad1GP"
      },
      "source": [
        "### 2. Eliminando outliers [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXTpIi1Bo2KG"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://media.licdn.com/dms/image/C5612AQGdXKCka7HumA/article-cover_image-shrink_600_2000/0/1520056407281?e=2147483647&v=beta&t=VZcfjjzjK4LxXdZkSu1KisWC0Ry8bk4tPCn3R8aYdNM\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqH4t-Jvj05"
      },
      "source": [
        "#### 2.1 Creando la clase IQR [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtCQGHN_mzEp"
      },
      "source": [
        "Entre las falencias de los datos, Mr. Cheems le comenta que a veces los operadores no ingresan el precio correcto de los productos. Mr. Cheems le comenta que se dio cuenta de este fen√≥meno porque hay productos con precios exager√°damente altos o bajos. Por lo cual usted decide eliminar outliers del dataframe a traves del rango intercuartil el cual cuenta con los siguientes pasos:\n",
        "\n",
        "1. Calcular el primer cuartil $Q1$ y el tercer cuartil $Q3$. Hint: utilice el m√©todo `quantile()`\n",
        "\n",
        "2. Calcular el rango intercuartil (RIC): $RIC = Q3 - Q1$\n",
        "\n",
        "3. Calcular los l√≠mites para identificar outliers:\n",
        " - L√≠mite inferior: $~~Q1 - \\lambda \\cdot RIC$\n",
        " - L√≠mite superior: $~~Q3 + \\lambda \\cdot RIC$\n",
        "\n",
        "4. Eliminar outliers: Los outliers son los datos que est√°n por debajo del l√≠mite inferior o por encima del l√≠mite superior.\n",
        "\n",
        "\n",
        "Para realizar dicha tarea, usted decide crear una clase llamada `IQR()` utilizando `BaseEstimator` y `TransformerMixin` para realizar una transformaci√≥n de cada una de las columnas num√©ricas del DataFrame utilizando `ColumnTransformer()` m√°s tarde. Considere que lambda debe ser $\\lambda$ un par√°metro a definir por el usuario.\n",
        "\n",
        "**Hint:** tome como referencia el siguiente [enlace](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer).\n",
        "\n",
        "**Nota:** No modificar el m√©todo set_output de la clase IQR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqK6AZnuhmL"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70CGFkRScKKP"
      },
      "outputs": [],
      "source": [
        "class IQR(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, cte=1.5):\n",
        "    self.cte = cte # cte representa lambda que define el usuario\n",
        "                   # por defecto es 1.5 (Ley de Tukey) y se√±ala que ese ser√≠a el l√≠mite\n",
        "                   # para considerar un punto como outlier en base a una convenci√≥n \"general\"\n",
        "                   # respecto de estos rangos pero que de todos modos uno podr√≠a definirlo\n",
        "                   # si tiene nociones reales de como se mueven los rangos para considerar \n",
        "                   # los outliers.\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y=None): # Ac√° se aplica lo de la gu√≠a\n",
        "    # hay que obtener solo columnas num√©ricas\n",
        "    self.num_columns = X.select_dtypes(include='number').columns # lista de columnas del dataframe. \n",
        "                                                                 # Por como ser√° el pipeline, \n",
        "                                                                 # deberian ser las columnas \n",
        "                                                                 # num√©ricas. Adem√°s, me aseguro\n",
        "                                                                 # que solo sean num√©ricas.\n",
        "\n",
        "    # No es necesario self.X = X ya que no se busca guardar el dataframe, solo calcular los l√≠mites\n",
        "    self.q1 = X[self.num_columns].quantile(0.25) # Q1\n",
        "    self.q3 = X[self.num_columns].quantile(0.75) # Q3\n",
        "    self.ric = self.q3 - self.q1 # RIC\n",
        "\n",
        "    self.lim_inf = self.q1 - self.ric*self.cte #lim. inferior\n",
        "    self.lim_sup = self.q3 + self.ric*self.cte #lim. superior\n",
        "\n",
        "    return self\n",
        "    \n",
        "\n",
        "  def transform(self, X): # de lo que entend√≠ de la gu√≠a es que ac√°, X se transforma con lo que aprendi√≥ en fit (los l√≠mites)\n",
        "                          # o sea ac√° se redefinen los outliers\n",
        "    \n",
        "    X_no_outlier = X.copy()\n",
        "    \n",
        "    mask = (( X_no_outlier[self.num_columns] < self.lim_inf) |\n",
        "            ( X_no_outlier[self.num_columns] > self.lim_sup))\n",
        "\n",
        "    # Enmascarar a NaN en lugar de eliminar el registro porque esto en el fondo\n",
        "    # generaria que distintas variables tuvieran distinta cantidad de registros\n",
        "\n",
        "    X_no_outlier.loc[:, self.num_columns] =  X_no_outlier[self.num_columns].mask(mask, other=np.nan)\n",
        "    return  X_no_outlier\n",
        "\n",
        "  def set_output(self,transform='default'):\n",
        "    #No modificar esta funci√≥n\n",
        "    return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pse94ohOm1um"
      },
      "source": [
        "#### 2.2 Creaci√≥n del Pipeline [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWWiGA5m_Hj"
      },
      "source": [
        "Para comenzar introduci√©ndose en el uso de pipeline, usted decide definir un pipeline con el Transformer previamente definido. Adem√°s, usted decide visualizar c√≥mo cambia la distribuci√≥n de las variables Precio y Cantidad antes y despues de aplicar IQR. Para ello, usted aplica los siguientes pasos:\n",
        "\n",
        "- Definir un pipeline llamado `numeric_transformations` para las variables precio y cantidad con la transformaci√≥n IQR. [0.1 puntos]\n",
        "- Defina un column transformer que aplique `numeric_transformations` para las variables num√©ricas y `passthrough` para las variables categ√≥ricas. Adicionalmente, fije el par√°metro `verbose_feature_names_out` en `False`. Ver hint al final [0.1 puntos]\n",
        "- Defina el dataframe `df_iqr` aplicado el column transformer a los datos proporcionados por Mr. Cheems considerando un valor de $\\lambda$ que tenga un desempe√±o aceptable para ambas variables. [0.1 puntos]\n",
        "- Usar `explore_data` en `df_retail` y en `df_iqr`.  [0.1 puntos]\n",
        "- Reportar los cambios observados en la distribuci√≥n de las variables. ¬øQu√© sucede al aumentar el valor de lambda? [0.1 puntos]\n",
        "\n",
        "\n",
        "**Hint:** El transformador `passthrough` est√° predefinido y es una opci√≥n que puedes usar para las columnas que no deseas transformar. Al especificar 'passthrough' para una parte de tu ColumnTransformer, las columnas correspondientes pasar√°n a trav√©s del ColumnTransformer sin ninguna modificaci√≥n. El siguiente [enlace](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) le puede ser √∫til.\n",
        "\n",
        "**Nota:** Mantenga el m√©todo set_output del column transformer con la transformaci√≥n `pandas` para obtener un dataframe una vez aplicado el column transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeizZcLuabD"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF24vWb4GwLo"
      },
      "source": [
        "Ap√≥yese de la siguiente estructura para su respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaSuz2NSn7g6"
      },
      "outputs": [],
      "source": [
        "# Lambda de 1.5 (Ley de Tukey) - mantenemos el mismo valor de cte que en la clase para efectos de comparaci√≥n y porqupe es el de la ley de Tukey por defecto\n",
        "\n",
        "# Definicion las variables que pasar√°n por cada pipeline\n",
        "numerical_columns = [\"Quantity\", \"Price\"]\n",
        "categorical_columns = [\"Invoice\", \"StockCode\" ,\"Description\", \"InvoiceDate\", \"Customer ID\" ,\"Country\"]\n",
        "\n",
        "# Definicion del pipeline\n",
        "numeric_transformations = Pipeline([(\"iqr_transformer\", IQR(cte=1.5))]) # el pipe espera una lista con tuplas (nombre, transformador)\n",
        "\n",
        "# ColumnTransformer\n",
        "column_transformer = ColumnTransformer([('numerical', numeric_transformations, numerical_columns),\n",
        "                                        ('categorical', \"passthrough\", categorical_columns) # dejo las categoricas con el pss porque aun no defino como las voy a tratar\n",
        "                                        ],                                                   \n",
        "                                        verbose_feature_names_out=False)\n",
        "\n",
        "column_transformer.set_output(transform='pandas') # esto es para asegurarnos que el output devuelva un dataframe y no un array\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "\n",
        "df_iqr = column_transformer.fit_transform(df_retail) # .fit() devuelve objeto entrenado no el dataframe transformado\n",
        "                                                    # por eso se usa fit_trasnform que si devuelve el dataframe transformado\n",
        "\n",
        "# Gr√°ficos\n",
        "print(\"Dataframe original, cte=1.5\")\n",
        "explore_data(df_retail, \"df_retail\") # Plot inicial sin suavizado logar√≠tmico\n",
        "\n",
        "print(\"Dataframe tras aplicar IQR, cte=1.5\")\n",
        "explore_data(df_iqr, \"df_iqr\") # Plot IQR sin suavizado logar√≠tmico\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Valores nulos en el dataframe inicial, cte=1.5\")\n",
        "df_retail.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lamda de 3 \n",
        "\n",
        "# Definicion las variables que pasar√°n por cada pipeline\n",
        "numerical_columns = [\"Quantity\", \"Price\"]\n",
        "categorical_columns = [\"Invoice\", \"StockCode\" ,\"Description\", \"InvoiceDate\", \"Customer ID\" ,\"Country\"]\n",
        "\n",
        "# Definicion del pipeline\n",
        "numeric_transformations = Pipeline([(\"iqr_transformer\", IQR(cte=3))]) # el pipe espera una lista con tuplas (nombre, transformador)\n",
        "\n",
        "# ColumnTransformer\n",
        "column_transformer = ColumnTransformer([('numerical', numeric_transformations, numerical_columns),\n",
        "                                        ('categorical', \"passthrough\", categorical_columns) # ac√° se pudo haber usado remainder si es que\n",
        "                                        ],                                                    # no se especificaban las variables no num√©ricas \n",
        "                                        verbose_feature_names_out=False)\n",
        "\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "\n",
        "df_iqr = column_transformer.fit_transform(df_retail) # .fit() devuelve objeto entrenado no el dataframe trasnformado, por eso se usa fit_trasnform y no .fit\n",
        "\n",
        "# Gr√°ficos\n",
        "print(\"Dataframe tras aplicar IQR, cte=3\")\n",
        "explore_data(df_iqr, \"df_iqr\") # Plot IQR sin suavizado logar√≠tmico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Valores nulos en el dataframe inicial, cte=3\")\n",
        "df_retail.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lambda de 5 \n",
        "\n",
        "# Definicion las variables que pasar√°n por cada pipeline\n",
        "numerical_columns = [\"Quantity\", \"Price\"]\n",
        "categorical_columns = [\"Invoice\", \"StockCode\" ,\"Description\", \"InvoiceDate\", \"Customer ID\" ,\"Country\"]\n",
        "\n",
        "# Definicion del pipeline\n",
        "numeric_transformations = Pipeline([(\"iqr_transformer\", IQR(cte=5))]) # el pipe espera una lista con tuplas (nombre, transformador)\n",
        "\n",
        "# ColumnTransformer\n",
        "column_transformer = ColumnTransformer([('numerical', numeric_transformations, numerical_columns),\n",
        "                                        ('categorical', \"passthrough\", categorical_columns) # ac√° se pudo haber usado remainder si es que\n",
        "                                        ],                                                    # no se especificaban las variables no num√©ricas \n",
        "                                        verbose_feature_names_out=False)\n",
        "\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "\n",
        "df_iqr = column_transformer.fit_transform(df_retail) # .fit() devuelve objeto entrenado no el dataframe transformado, \n",
        "                                                        #por eso se usa fit_trasnform y no .fit\n",
        "\n",
        "# Gr√°ficos\n",
        "print(\"Dataframe tras aplicar IQR, cte=5\")\n",
        "explore_data(df_iqr, \"df_iqr\") # Plot IQR sin suavizado logar√≠tmico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Valores nulos en el dataframe inicial, cte=5\")\n",
        "df_retail.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKnc6UcsDkm"
      },
      "source": [
        "**RESPUESTA**\n",
        "\n",
        "Lo que vimos es que al variar la cte (Œª) entre 1.5, 3 y 5 cambia harto la distribuci√≥n porque b√°sicamente est√°s definiendo qu√© tan ‚Äútolerante‚Äù eres con los valores extremos. Mientras m√°s grande es Œª, m√°s se ampl√≠a el rango en que se consideran datos ‚Äúnormales‚Äù, entonces menos observaciones se marcan como outliers y, en consecuencia, menos datos terminan como nulos tras aplicar la regla del IQR. Ejemplo concreto con Œª = 1.5, en Quantity se generan m√°s de 33 mil nulos, mientras que con Œª = 3 bajan a 26 mil y con Œª = 5 caen a 18 mil aprox. Lo mismo en Price, donde pasamos de 40 mil nulos (Œª = 1.5) a 18 mil (Œª = 3) y 11 mil (Œª = 5). O sea, a medida que sube Œª, el set se ‚Äúrelaja‚Äù y se eliminan menos observaciones.\n",
        "\n",
        "Entonces, un Œª chico (ej. 1.5) es m√°s estricto y limpia fuerte las colas, un Œª grande (ej. 5) deja pasar muchos m√°s valores como ‚Äúv√°lidos‚Äù aunque est√©n alejados de la mediana. Lo clave es que la elecci√≥n de Œª no sea al azar, sino que dependa del conocimiento de la variable y del contexto de negocio/tema. Por algo Tukey propuso la regla de 1.5 como referencia general (‚Äúla pulgada‚Äù), pero igual puede ajustarse si tenemos fundamentos s√≥lidos para hacerlo.\n",
        "\n",
        "Si miramos los gr√°ficos, el contraste queda clar√≠simo pues en el dataset original (df_retail) tanto Price como Quantity aparecen ultra sesgados a la derecha, con un cerro de observaciones acumuladas en valores muy bajos y colas largu√≠simas que llegan hasta 10k‚Äì18k. Esto hace que la mayor√≠a de los bins se aplasten y casi no se aprecien los patrones internos. Cuando aplicamos IQR con Œª = 1.5, esas colas se recortan fuerte y las escalas del eje x bajan harto, dejando las distribuciones mucho m√°s ‚Äúvisibles‚Äù en el rango 0‚Äì8 en Price y 0‚Äì25 en Quantity. Con Œª = 3 las colas vuelven a alargarse un poco y se empieza a ver m√°s dispersi√≥n, y con Œª = 5 ya casi se parece a la forma original, porque entran nuevamente valores que antes se hab√≠an tratado como outliers. Entonces, a mayor Œª, aumenta el rango visible de los ejes en los gr√°ficos porque se considera ‚Äúnormal‚Äù una mayor cantidad de valores extremos, mientras que con Œª m√°s chico el corte es m√°s duro y se pierde parte de la cola."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5s4dqMYCbJ"
      },
      "source": [
        "### 3. Agregando un imputer al pipeline [1.0 puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bc9fFeXp-At"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://media.makeameme.org/created/hmm-there-is.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uugEdc26vJ5N"
      },
      "source": [
        "Para continuar con la limpieza del dataframe usted decide imputar los datos nulos de las variables num√©ricas, para lo cual decide realizar las siguientes tareas:\n",
        "\n",
        "1. Crear un pipeline para variables categ√≥ricas llamado `categoric_transformations` con un paso llamado `mode_imputer`, en el cual se imputen los datos faltantes por la categor√≠a m√°s frecuente.\n",
        "2. Agregar al pipeline `numeric_transformations` un paso llamado `mean_imputer`, en el cual se imputen los datos por la media usando [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) [0.1 puntos]\n",
        "3. Crear y aplicar un `ColumnTransformer` actualizado con los pipelines `categoric_transformations` y `numeric_transformations` a `df_retail`, creando un dataframe llamado `df_mean_imputer`. [0.1 puntos]\n",
        "4. Comparar los resultados de `explore_data` en `df_mean_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [0.2 puntos]\n",
        "5. Cambiar el imputer de `numeric_transformations` por [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) y definir un nuevo dataframe llamado `df_knn_imputer`, aplicando el nuevo ColumnTransformer a `df_retail`. En caso de los tiempos de ejecuci√≥n sean altos puede probar a reducir el par√°metro `n_neighbors`. [0.1 puntos]\n",
        "6. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [0.2 puntos]\n",
        "7. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_mean_imputer`. ¬øCu√°l m√©todo de imputaci√≥n es mejor? Deje el m√©todo escogido en el ColumnTransformer. [0.2 puntos]\n",
        "\n",
        "**Nota: Fije el par√°metro verbose_feature_names_out en `False` y utilice el m√©todo set_output con transformaci√≥n `pandas` en cada ColumnTransformer para obtener como salida un dataframe.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVUdZZxuo4o"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ESTA CELDA TARDA ENTRE 20 Y 30 MIN EN CORRER SEG√öN COMPUTADOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# Defino los tipos de columnas\n",
        "# Estos son los tipos de datos que tenemos en la base category, object, datetime64[ns] y float64 entonces definimos por separado la agrupaci√≥n para que despu√©s se apliquen las imputaciones\n",
        "# correspondientes\n",
        "\n",
        "numeric_cols   = df_retail.select_dtypes(include='float64').columns.tolist()\n",
        "categoric_cols = df_retail.select_dtypes(include=['object','category','datetime64[ns]']).columns.tolist()\n",
        "\n",
        "# PARTE 1 \n",
        "\n",
        "categoric_transformations = Pipeline(steps=[('mode_imputer', SimpleImputer(strategy='most_frequent'))]) # Aqu√≠ estamos definiendo el pipe que en el fondo imputar√° a las variables categ√≥ricas \n",
        "                                                                                                        # el valor m√°s frecuente de la categor√≠a\n",
        "                                                                                                        # Si bien desde un punto de vista de \"imputaci√≥n\" esto se podr√≠a hacer, no significa\n",
        "                                                                                                        # que esta sea la forma correcta de imputar todas estas clases de datos. Estrictamente \n",
        "                                                                                                        # deber√≠amos analizar variable por variable e imputar seg√∫n correspnda a la naturaleza \n",
        "                                                                                                        # de la misma y lo que se necesite encontrar por decir.\n",
        "                                                                                                        # Ac√° no se explicitaba pero igual usamos SimpleImputer porque de esta forma salia bien.\n",
        "\n",
        "# PARTE 2 \n",
        "# Aqu√≠ piden agregar un paso de \"mean_imputer\" en el pipe de \"numeric_transformations\" por lo que se entiende que debemos mantener el mismo pipe que teniamos definido de antes y a este\n",
        "# agregarle el paso que nos comentan. No se esta pidiendo que redefinamos o que creemos un nuevo pipe por decir, m√°s solo agregar el paso.\n",
        "\n",
        "numeric_transformations = Pipeline([(\"iqr_transformer\", IQR(cte=1.5)), (\"mean_imputer\", SimpleImputer(strategy=\"mean\"))]) # El segundo parentesis, por decir, es el paso que se agrega ac√° porque \n",
        "                                                                                                                        # el resto venia de la definici√≥n inical de este pipe de una secci√≥n previa\n",
        "\n",
        "# PARTE 3\n",
        "\n",
        "ct_actualizado = ColumnTransformer( # con esto estoy creando el columntransformer para aplicar en la bbdd con el nombre de ct_actualizado\n",
        "    transformers=[ #aqui defino las diferentes transformaciones que voy a a plicar sobre las diferentes columnas o tipos de datos en vd\n",
        "        ('categoric_transformations', categoric_transformations, categoric_cols), # se compone como del nombre de la transformacion + el pipeline + las colunas donde lo aplicare que en este caso las definimos al inicio segun el tipo de datos que son\n",
        "        ('numeric_transformations',   numeric_transformations,   numeric_cols)\n",
        "    ],\n",
        "    verbose_feature_names_out=False, # esto que tmb aplicamos antes es como para que las columnas no queden renombradas\n",
        "    remainder='passthrough' # esto es como para que no tire error/levante alerta si hay columnas en el df que no se consideren dentro de las transformaciones, entonces as√≠ los deje tan cual\n",
        ").set_output(transform='pandas') # por defecto ct me devuelve un array pro yo quiero el dataframe de pandas entonces lo fuerzo a que eso pase por decri\n",
        "\n",
        "df_mean_imputer = ct_actualizado.fit_transform(df_retail) # con esto obtengo el df transformado pero las col no necesarimente est√°n en el mismo orden\n",
        "df_mean_imputer = df_mean_imputer[df_retail.columns] # Con esto puedo volverlas al mismo orden del inicio\n",
        "\n",
        "\n",
        "# PARTE 4 \n",
        "# ahora voy a comparar los datos de \"df_mean_imputer\" y \"df_iqr\" seg√∫n lo definido al inicio en el \"explore_data\"\n",
        "\n",
        "print(\"PARTE 4: explore_data(df_mean_imputer) vs explore_data(df_iqr)\")\n",
        "explore_data(df_mean_imputer, \"df_mean_imputer\")\n",
        "explore_data(df_iqr, \"df_iqr\")\n",
        "\n",
        "# PARTE 5 \n",
        "\n",
        "# aqui redefino el pipe (en verdad cree uno nuevo por knn) con el nuevo imputador\n",
        "numeric_transformations_knn = Pipeline([(\"iqr_transformer\", IQR(cte=1.5)), (\"knn_imputer\", KNNImputer(n_neighbors=2, weights=\"uniform\", metric=\"nan_euclidean\"))])\n",
        "\n",
        "\n",
        "ct_knn = ColumnTransformer( # Defino el columntransform\n",
        "    transformers=[\n",
        "        ('categoric_transformations', categoric_transformations, categoric_cols), # defino los cambios sobre los tipos de varibales\n",
        "        ('numeric_transformations_knn',   numeric_transformations_knn,   numeric_cols),\n",
        "    ],\n",
        "    verbose_feature_names_out=False, # evito que cambie el nombre de la columna\n",
        "    remainder='passthrough' # aqqui para que no modifique o tire error si hay variables que no tendran modificacion\n",
        ").set_output(transform='pandas') # forzar al df de pandas en lugar del array\n",
        "\n",
        "df_knn_imputer = ct_knn.fit_transform(df_retail) # aqui lo aplico ya a df_retail\n",
        "df_knn_imputer = df_knn_imputer[df_retail.columns] # reordeno las columnas nuevamente\n",
        "\n",
        "\n",
        "# PARTE 6\n",
        "# aca comparo tambi√©n los diferentes dataframes con las transformaciones de cada uno\n",
        "\n",
        "print(\"PARTE 6: explore_data(df_knn_imputer) vs explore_data(df_iqr)\")\n",
        "explore_data(df_knn_imputer, \"df_knn_imputer\")\n",
        "explore_data(df_iqr, \"df_iqr\")\n",
        "\n",
        "# PARTE 7\n",
        "# Primero tengo que definir la media y desviaion estandar de las varibales numericas, porque este ser√° como mi \n",
        "# baseline para comparar que imputador es mejor\n",
        "\n",
        "ref_mean = df_retail[numeric_cols].mean()\n",
        "ref_std  = df_retail[numeric_cols].std(ddof=1)\n",
        "\n",
        "# Hago como un c√°lculo de \"distancias simples\" pero en valor absoluto (|Œî media| + |Œî std|). \n",
        "# Mientras menor, mejor por que en el fondo implicar√≠a que esta m√°s cerca de lo \"real\".\n",
        "dist_mean = (ref_mean - df_mean_imputer[numeric_cols].mean()).abs().sum() \\\n",
        "          + (ref_std  - df_mean_imputer[numeric_cols].std(ddof=1)).abs().sum()\n",
        "\n",
        "dist_knn  = (ref_mean - df_knn_imputer[numeric_cols].mean()).abs().sum() \\\n",
        "          + (ref_std  - df_knn_imputer[numeric_cols].std(ddof=1)).abs().sum()\n",
        "\n",
        "print(\"PARTE 7: KNN vs MEAN (comparaci√≥n cuantitativa simple)\")\n",
        "print(f\"Distancia media+std vs original -> MEAN: {dist_mean:.6f} | KNN: {dist_knn:.6f}\")\n",
        "\n",
        "# aca comparo los resultados\n",
        "print(\"explore_data: df_knn_imputer vs df_mean_imputer\")\n",
        "explore_data(df_knn_imputer, \"df_knn_imputer\")\n",
        "explore_data(df_mean_imputer, \"df_mean_imputer\")\n",
        "\n",
        "# hago un if que me deje automaticamente elegir con cual nos vamos a quedar\n",
        "if dist_knn <= dist_mean:\n",
        "    ct_final = ct_knn\n",
        "    df_imputer_final = df_knn_imputer\n",
        "    metodo_elegido = \"KNNImputer\"\n",
        "else:\n",
        "    ct_final = ct_actualizado\n",
        "    df_imputer_final = df_mean_imputer\n",
        "    metodo_elegido = \"SimpleImputer(mean)\"\n",
        "\n",
        "print(f\"M√©todo de imputaci√≥n escogido: {metodo_elegido}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RESPUESTAS A LAS PREGUNTAS 4, 6 Y 7**\n",
        "\n",
        "4 --> Al comparar df_mean_imputer con df_iqr se nota que el primero queda sin nulos y mucho m√°s ‚Äúlimpio‚Äù para modelar, pero al mismo tiempo sus distribuciones aparecen m√°s apretadas en el centro y con colas cortas, porque al imputar con la media todos los huecos se van al valor central y eso reduce la dispersi√≥n. En cambio, con df_iqr quedan varios nulos, sobre todo en Quantity y Price, ya que ah√≠ se marcan outliers seg√∫n el rango intercuart√≠lico, pero lo interesante es que en los gr√°ficos se alcanzan a ver colas mucho m√°s largas y rangos m√°s amplios: en Price la escala llega hasta 16 en vez de 7.5 y en Quantity hasta 60 en vez de 25, lo que refleja que el IQR mantiene la variabilidad y la asimetr√≠a real de los datos en vez de suavizarla. En la pr√°ctica entonces, el mean_imputer da un dataset m√°s estable y sin faltantes, aunque un poco artificial en su forma, mientras que el IQR conserva mejor la heterogeneidad del negocio pero al costo de tener que decidir despu√©s c√≥mo tratar esos nulos. Adem√°s, al variar la constante Œª lo que pasa es que con valores m√°s chicos (ej. 1.5) se eliminan m√°s datos y se cortan fuerte las colas, mientras que con valores grandes (ej. 5) entran m√°s observaciones dentro del rango ‚Äúv√°lido‚Äù, se generan menos nulos y se recupera la forma con colas largas que vimos en los histogramas.\n",
        "\n",
        "6 --> Cuando comparamos el df_knn_imputer con el df_iqr se nota que con KNN el dataset queda sin nulos y las distribuciones se ven mucho m√°s ‚Äúcompactas‚Äù, porque los valores faltantes se imputan a partir de vecinos y eso hace que las formas se mantengan ordenadas y sin tantos vac√≠os, mientras que con IQR aparecen hartos nulos en Quantity y Price y eso se refleja en histogramas con colas m√°s largas y rangos m√°s amplios, por ejemplo en Price el KNN se queda m√°s acotado hasta 7 y en el IQR se estira hasta 16, y en Quantity pasa algo similar, donde KNN mantiene el rango hasta aprox. 25 mientras que el IQR llega a m√°s de 60; en el fondo KNN te entrega un dataset operativo y sin huecos pero suaviza bastante la dispersi√≥n real, mientras que IQR conserva la variabilidad y los extremos del negocio aunque a costa de dejar faltantes que despu√©s hay que tratar, o sea es como m√°s fiel a la data original pero menos pr√°ctico de usar tal cual.\n",
        "\n",
        "7 --> Comparando df_knn_imputer con df_mean_imputer nos quedamos con KNN pues ambos dejan cero nulos, pero el mean empuja los huecos al centro y te ‚Äúaplana‚Äù un poco las colas (queda todo m√°s parejito pero medio artificial), en cambio KNN rellena usando vecinos y la forma de las distribuciones se ve m√°s parecida a la original, con la variabilidad donde corresponde; de hecho en la comparaci√≥n cuantitativa que corrimos la distancia media+std vs el original dio KNN = 133.385891 vs MEAN = 134.122979, o sea KNN queda m√°s cerca del original (poquito, pero gana), y visualmente tambi√©n se nota menos sesgo al centro. As√≠ que, por fidelidad y porque no mete tanto ‚Äúpromedio‚Äù donde no toca, escogemos KNN y lo dejamos como m√©todo en el ColumnTransformer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuUiW-9YYZ3"
      },
      "source": [
        "### 4. Creaci√≥n de nuevas features [2.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQSuoL5mubnA"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=250 src=\"https://miro.medium.com/max/1000/1*JtTWgAcfVTWV8OTjT47Atg.jpeg\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-yHP5oIvzFS"
      },
      "source": [
        "#### 4.1 Definicion de LRMFP [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe0V2CnZY8Bc"
      },
      "source": [
        "Dado que Mr. Lepin est√° interesado en obtener nuevos atributos relevantes para su negocio, su equipo de expertos sugiere la construcci√≥n de variables **LRMFP**, las que se construyen en base a las siguientes definiciones:\n",
        "\n",
        "- **Length (L)**: Intervalo de tiempo, en d√≠as, entre la primera y la √∫ltima visita del cliente. Mientras mas grande sea el valor, mas fiel es el cliente.\n",
        "\n",
        "- **Recency (R)**: Indica hace cuanto tiempo el cliente realizo su ultima compra. Notar que para este caso, mientras mas grande es el valor, menos interes posee el usuario para repetir una compra en uno de los locales. **Considere \"hoy\" como la fecha mas reciente del dataset**.\n",
        "\n",
        "- **Monetary (M)**: El t√©rmino \"monetario\" se refiere a la cantidad media de dinero gastada por cada visita del cliente durante el per√≠odo de observaci√≥n y refleja la contribuci√≥n del cliente a los ingresos de la empresa.\n",
        "\n",
        "- **Frequency (F)**: Se refiere al n√∫mero total de visitas del cliente durante el periodo de observaci√≥n. Cuanto mayor sea la frecuencia, mayor ser√° la fidelidad del cliente.\n",
        "\n",
        "- **Periodicity (P)**: Representa si los clientes visitan las tiendas con regularidad.\n",
        "\n",
        "$$Periodicity(n)=std(IVT_1, ..., IVT_n)$$\n",
        "\n",
        "Donde $IVT$ denota el tiempo entre visitas y n representa el n√∫mero de valores de tiempo entre visitas de un cliente.\n",
        "\n",
        "\n",
        "$$IVT_i=date\\_diff(t_{i+1},t)$$\n",
        "\n",
        "En base a las definiciones se√±aladas, dise√±e una funci√≥n que permita obtener las caracter√≠sticas **LRMFP** recibiendo un DataFrame como entrada. Para esto, no estar√° permitido el uso de iteradores, utilice todas las herramientas que les ofrece `pandas` para realizar esto.\n",
        "\n",
        "Una referencia que le puede ser √∫til es el [documento original](https://www.researchgate.net/publication/315979555_LRFMP_model_for_customer_segmentation_in_the_grocery_retail_industry_a_case_study) en donde se propone este m√©todo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bee8d549c7c043a5b0cafae0543afadf",
        "deepnote_cell_height": 212.6666717529297,
        "deepnote_cell_type": "markdown",
        "id": "L7ZwWJxhXMfk",
        "tags": []
      },
      "source": [
        "**<u>Formato</u> del Resultado Esperado:**\n",
        "\n",
        "| Customer ID | Length | Recency | Frequency | Monetary | Periodicity |\n",
        "|------------:|-------:|--------:|----------:|---------:|------------:|\n",
        "|   12346.0   |    294 |      67 |        46 |   -64.68 |        37.0 |\n",
        "|   12347.0   |     37 |       3 |        71 |  1323.32 |         0.0 |\n",
        "|   12349.0   |    327 |      43 |       107 |  2646.99 |        78.0 |\n",
        "|   12352.0   |     16 |      11 |        18 |   343.80 |         0.0 |\n",
        "|   12356.0   |     44 |      16 |        84 |  3562.25 |        12.0 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3c7f8a4a06a44cbd8d50e8a4decf4c71",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "6GaQZaMXXMfk",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_features(dataframe_in):\n",
        "    \n",
        "    copia = dataframe_in.copy()\n",
        "\n",
        "    copia = copia.sort_values([\"Customer ID\", \"InvoiceDate\"]) # se ordena porque ser√° m√°s √∫til para adelante\n",
        "\n",
        "    # PARA LENGHT\n",
        "\n",
        "    # Se obtienen columnas customer id, primera_visita y ultima_visita\n",
        "    len = copia.groupby(\"Customer ID\", observed=True)[\"InvoiceDate\"].agg(primera_visita=\"min\", ultima_visita=\"max\").reset_index() # esto es un dataframe\n",
        "   \n",
        "    Lenght = (len[\"ultima_visita\"] - len[\"primera_visita\"]).dt.days # diferencia en d√≠as: esto no es un dataframe\n",
        "    \n",
        "    # PARA RECENCY\n",
        "    Recency = np.abs((len[\"ultima_visita\"] - pd.to_datetime(\"today\")).dt.days) # valor absoluto: esto no es un dataframe\n",
        "\n",
        "    # PARA MONETARY\n",
        "    copia[\"dinero_gastado\"] =  copia[\"Quantity\"] * copia[\"Price\"]\n",
        "    \n",
        "    Monetary = copia.groupby(\"Customer ID\", observed=True)[\"dinero_gastado\"].mean().round(2).fillna(0).reset_index(name=\"Monetary\") \n",
        "    # promedio gastado por cliente: esto es un dataframe\n",
        "    # ac√° me di cuenta que hay un producto con precio 0.0, luego la multiplicacion da nan\n",
        "    \n",
        "    # PARA FREQUENCY\n",
        "    copia[\"fecha\"] = copia[\"InvoiceDate\"].dt.normalize() # convierte a midnight pero quedan los dd/mm/aaaa\n",
        "\n",
        "    Frequency = copia.groupby(\"Customer ID\", observed=True)[\"fecha\"].nunique().reset_index(name=\"Frequency\") \n",
        "    # dataframe con  columnas customer id, fecha y frequency\n",
        "\n",
        "    # PARA PERIODICIDAD\n",
        "    \n",
        "    copia[\"IVT\"] = copia.groupby(\"Customer ID\", observed=True)[\"InvoiceDate\"].diff().dt.days # obtenemos la diferencia de dias para cada cliente\n",
        "\n",
        "    periodicidad = copia.groupby(\"Customer ID\", observed=True)[\"IVT\"].std().round(1).fillna(0).reset_index(name=\"Periodicity\") # se aplica desviacion estandar y reseteamos index\n",
        "    # periodicidad es dataframe con columnas customer id y periodicidad (periodicity)\n",
        "\n",
        "    # si un cliente visita 1 solo dia, no hay std -> se tiene un nan\n",
        "    # Si un cliente visita 2 dias, hay solo un intervalo -> no hay desviaci√≥n estandar -> arroja nan\n",
        "    \n",
        "    dataframe_out = pd.DataFrame({\n",
        "        \"Customer ID\": len[\"Customer ID\"],\n",
        "        \"Lenght\": Lenght,\n",
        "        \"Recency\": Recency,\n",
        "        \"Monetary\": Monetary[\"Monetary\"],\n",
        "        \"Frequency\": Frequency[\"Frequency\"] ,\n",
        "        \"Periodicity\": periodicidad[\"Periodicity\"]\n",
        "\n",
        "    })\n",
        "    return dataframe_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddL8wThv36t"
      },
      "source": [
        "#### 4.2 Agregando las custom features [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehLWiQzjwDm-"
      },
      "source": [
        "Ahora, usted decide agregar al pipeline las nuevas variables creadas, para lo cual realiza las siguientes tareas:\n",
        "\n",
        "1. Cree un nuevo pipeline llamado `retail_pipeline` que encapsule el ColumnTransformer y calcule las LRMFP. El primer paso del pipeline ll√°melo  `col_tranformer` y el segundo paso ll√°melo `custom_features`, incorpora las nuevas variables al dataframe. Hint: les puede ser √∫til investigar [este](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) m√©todo. [0.1 puntos]\n",
        "2. Aplicar el pipeline actualizado a los datos proporcionados por Mr. Cheems, creando un nuevo dataframe llamado `df_custom`. [0.1 puntos]\n",
        "3. Explorar la distribuci√≥n de las nuevas variables con `explore_data` y comentar brevemente (2-3 l√≠neas) caracter√≠sticas de cada custom feature. [0.5 puntos]\n",
        "5. Entregar un insight para el negocio en base a las nuevas variables. [0.3 puntos]\n",
        "\n",
        "**Nota:** Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVCGxPgtwFsk"
      },
      "source": [
        "**Respuesta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ESTA CELDA TARDA EN CORRER ENTRE 25 A 35 MINUTOS SEGUN COMPUTADOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxILi3w0wE9Q"
      },
      "outputs": [],
      "source": [
        "custom_features_transformer = FunctionTransformer(custom_features) # uso del hint que pusieron\n",
        "\n",
        "retail_pipe = Pipeline(steps=[\n",
        "                                (\"col_transformer\", ct_knn), \n",
        "                                (\"custom_features\", custom_features_transformer)    \n",
        "                            ])                                                      # se escogi√≥ el col_trans de knn pq salio como el mejor para imputar\n",
        "\n",
        "df_custom = retail_pipe.fit_transform(df_retail) # ac√°, se realizan las imputaciones correspondientes con knn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definici√≥n de una nueva funcion explore_data\n",
        "\n",
        "def explore_custom(df, name):\n",
        "    \"\"\" Funci√≥n que recibe un dataframe, el cual debe contener las columnas\n",
        "    \"Lenght\", \"Recency\", \"Monetary\", \"Frequency\", \"Periodicity\" y \n",
        "    retorna un histograma de cada variable.\n",
        "\n",
        "    Par√°metros:\n",
        "    --------------\n",
        "    df: dataframe a tratar.\n",
        "\n",
        "    name: (str) Nombre del dataframe.\n",
        "\n",
        "    yscale: (str) Modifica la escala del eje y porque a veces no\n",
        "                  se puede graficar bien con plotly debido a la cantidad \n",
        "                  de los datos.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    pio.renderers.default = \"notebook\"\n",
        "\n",
        "    # Impresi√≥n de valores nulos por columna\n",
        "    print(\"Valores nulos por columna\")\n",
        "    print(df.isna().sum()) # deber√≠a dar 0 porque hubo imputaci√≥n en todas las variables.\n",
        "\n",
        "    # Histogramas\n",
        "\n",
        "    # Lenght\n",
        "\n",
        "    fig1 = px.histogram(\n",
        "        df, x = \"Lenght\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Length del dataframe {name}\",\n",
        "    )\n",
        "\n",
        "    fig1.show()\n",
        "\n",
        "    # Recency\n",
        "\n",
        "    fig2 = px.histogram(\n",
        "        df, x = \"Recency\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Recency del dataframe {name}\",\n",
        "    )\n",
        "\n",
        "    fig2.show()\n",
        "\n",
        "    # Monetary\n",
        "\n",
        "    fig3 = px.histogram(\n",
        "        df, x = \"Monetary\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Monetary del dataframe {name}\",\n",
        "    )\n",
        "\n",
        "    fig3.show()\n",
        "    \n",
        "    # Frequency\n",
        "\n",
        "    fig4 = px.histogram(\n",
        "        df, x = \"Frequency\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Frequency del dataframe {name}\",\n",
        "    )\n",
        "\n",
        "    fig4.show()\n",
        "\n",
        "    # Periodicidad\n",
        "\n",
        "    \n",
        "    fig4 = px.histogram(\n",
        "        df, x = \"Periodicity\", nbins=120,\n",
        "        title=f\"Distribuci√≥n de Periodicity del dataframe {name}\",\n",
        "    )\n",
        "\n",
        "    fig4.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explore_custom(df_custom, \"df_custom\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Descripci√≥n de las distribuciones de las variables LRMFP:\n",
        "\n",
        "**Lenght**: Se ve que hay una gran concentraci√≥n de los datos\n",
        "en los primeros bins, lo cual significa que hay una gran cantidad\n",
        "de clientes que fueron a comprar pocas veces, ya que la diferencia\n",
        "en d√≠as entre la √∫ltima visita y la primera visita es muy peque√±a.\n",
        "Sin embargo hay una gran cola larga a la derecha de la distribuci√≥n,\n",
        "dando cuenta que hay un gran n√∫mero de clientes que fueron fieles\n",
        "a la empresa durante el per√≠odode observaci√≥n, ya que hicieron compras/visitas por mucho tiempo, es decir, \n",
        "la diferencia en d√≠as entre su √∫ltima visita y primera visita es\n",
        "alta.\n",
        "\n",
        "**Recency**: Se observa una distribuci√≥n asim√©trica sesgada a la derecha. Tambi√©n nos fijamos en los valores del eje x, que indica\n",
        "que las fechas disponibles del dataset corresponden a un per√≠odo antiguo en comparaci√≥n a la fecha de hoy.\n",
        "El d√≠ficil interpretar si los clientes presentar√≠an inter√©s en comprar en el d√≠a de hoy ya que la data es antigua y se obtiene una Recency del orden de\n",
        "5* 10^3. Pero hay una gran cantidad de clientes que se concentra \n",
        "en los menores valores obtenidos de Recency: La mayor√≠a de los clientes se concentra\n",
        "en Recency $\\in$ [5400, 5450].\n",
        "Aproximadamente, hay un cuarto del total de clientes que fueron fieles en el per√≠odo de observaci√≥n.\n",
        "\n",
        "**Monetary**: Se observa una distribuci√≥n asim√©trica sesgada a la derecha,\n",
        "de lo que se desprende que casi la mitad de los clientes (~2000) hacen un aporte\n",
        "monetario promedio de 12 a 23 unidades monetarias, siendo\n",
        "el intervalo [15-17] el que posee m√°s clientes (~700) que realizan ese aporte en \n",
        "promedio. \n",
        "Hay pocos clientes (<100) que realizan en promedio grandes aportes monetarios a la empresa (m√°s de 40 unidades\n",
        "monetarias).\n",
        "\n",
        "**Frequency**: La mayor√≠a de los datos est√°n cargados a la izquierda, \n",
        "donde el rango de frecuencia [2,3] es el que m√°s posee conteos.\n",
        "Alrededor de 1514 clientes realizaron alrededor de 2 o 3 visitas en promedio.\n",
        "Luego, hay ~900 clientes que visitaron solo una vez y 781 clientes que visitaron entre 4 y 5 veces.\n",
        "\n",
        "La cantidad de clientes que visita m√°s de 5 veces la tienda, disminuye exponencialmente.\n",
        "\n",
        "\n",
        "**Periodicidad**: Hay una gran cantidad de clientes que presentan periodicidad nula.\n",
        "Esto es esperable ya que hay muchos clientes que solo visitaron la \n",
        "tienda 1 vez o 2 veces , luego el IVT es 0 o 1, intervalos que no dispersi√≥n.\n",
        "(y esos valores se fijaron como 0 para no remper la integridad del dataframe LRMFP).\n",
        "Por otra parte, hay clientes que presentan alta periocididad; hay una gran concentraci√≥n\n",
        "de clientes (~800) que presentan periodicidad $\\in$ [7,13]. Luego\n",
        "casi un cuarto de los clientes si visitaron la tienda con regularidad.\n",
        "\n",
        "\n",
        "**Insight**: Despu√©s de analizar la distribuci√≥n de las nuevas variables de la\n",
        "data disponible,\n",
        "el negocio si present√≥ una clientela fiel durante el per√≠odo de observaci√≥n, que visita en promedio \n",
        "2 a 5 veces y que en promedio realiza un aporte importante\n",
        "de ~15 unidades momenetarias, sin embargo se recomendar√≠a \n",
        "consultar con un experto de marketing \n",
        "o de negocios para que la empresa sea m√°s considerada con esos clientes\n",
        "para aumentar su leatad y tambi√©n se recomienda consultar con expertos sobre\n",
        "pol√≠ticas para capturar y evitar fugas de clientes que pueden ser ctalogados como ocasionales: frecuencia baja, aporte monetario moderado a la empresa y\n",
        "periodicidad nula.\n",
        "\n",
        "Ahora otro punto a considerar es que si se desean implementar ciertas pol√≠ticas con respecto a los clientes, deber√≠a usarse data m√°s actual\n",
        "para tomar decisiones informadas, ya que los contextos temporales de las compras pueden ser distintos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOV0y-e_lS39"
      },
      "source": [
        "### 5. MinMax Scaler [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T55ZgReXvjGe"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://i.imgflip.com/1fsprn.jpg\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk2R1kvuu-e"
      },
      "source": [
        "#### 5.1 Definici√≥n del Column Transformer [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "94c48775ecb4496d970fbd920f65c126",
        "deepnote_cell_height": 268.70001220703125,
        "deepnote_cell_type": "markdown",
        "id": "iWsfp1dKXMfo",
        "tags": []
      },
      "source": [
        "Construya una clase llamada `MinMax()` para realizar una transformaci√≥n de cada una de las columnas de un DataFrame utilizando `ColumnTransformer()`. Recuerde  usar `BaseEstimator` y `TransformerMixin`.\n",
        "\n",
        "\n",
        " Para esto considere que Min-Max escaler queda dada por la ecuaci√≥n:\n",
        "\n",
        "$$MinMax = \\dfrac{x-min(x)}{max(x) - min(x)}$$\n",
        "\n",
        "\n",
        "Consulte el siguiente [link](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer) si tiene dudas sobre la creaci√≥n de custom transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c087d1fa8aa94d7485fe1292bf628660",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "MUOLTWPDXMfo",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "07cb4dcf097c4c6baabb9ae2bda25caf",
        "deepnote_cell_height": 83.86666870117188,
        "deepnote_cell_type": "code",
        "id": "g15ZMCs-XMfo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MinMax(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Escala cada columna a [0, 1] con la f√≥rmula:\n",
        "        MinMax = (X - min_col) / (max_col - min_col)\n",
        "    - Deja NaN sin tocar (np.nanmin/np.nanmax ignoran NaN).\n",
        "    - Si max == min (columna constante), uso denominador 1 para evitar divisi√≥n por cero,\n",
        "      por lo que la columna resultar√° en ceros.\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Guardo si me pasaron un DataFrame para poder devolver el mismo tipo en transform()\n",
        "        self._is_df_ = isinstance(X, pd.DataFrame)\n",
        "        if self._is_df_:\n",
        "            # Si es DataFrame, recuerdo columnas e √≠ndice\n",
        "            self._cols_  = X.columns\n",
        "            self._index_ = X.index\n",
        "            # Convierto a ndarray float para calcular min/max/rango de forma homog√©nea\n",
        "            Xv = X.values.astype(float)\n",
        "        else:\n",
        "            # Si no es DataFrame, aseguro que sea un ndarray float\n",
        "            Xv = np.asarray(X, dtype=float)\n",
        "\n",
        "        # Calculo m√≠nimos por columna ignorando NaN\n",
        "        self.data_min_ = np.nanmin(Xv, axis=0)\n",
        "        # Calculo m√°ximos por columna ignorando NaN\n",
        "        self.data_max_ = np.nanmax(Xv, axis=0)\n",
        "        # Rango por columna (max - min)\n",
        "        self.range_    = self.data_max_ - self.data_min_\n",
        "\n",
        "        # Evito divisi√≥n por cero as√≠ que donde el rango sea 0, lo reemplazo por 1 (as√≠ toda la columna constante queda en 0 tras escalar)\n",
        "        self.range_safe_ = np.where(self.range_ == 0.0, 1.0, self.range_)\n",
        "\n",
        "        # Devuelvo self ajustado\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "        # Detecto si ahora me pasan DataFrame para preservar nombres/√≠ndice\n",
        "        is_df = isinstance(X, pd.DataFrame)\n",
        "        if is_df:\n",
        "            # Guardo columnas e √≠ndice actuales\n",
        "            cols = X.columns\n",
        "            idx  = X.index\n",
        "            # Trabajo internamente con ndarray float\n",
        "            Xv = X.values.astype(float)\n",
        "        else:\n",
        "            # Si me pasan ndarray, intento recuperar columnas guardadas en el fit\n",
        "            cols = getattr(self, \"_cols_\", None)\n",
        "            idx  = None\n",
        "            # Me aseguro de tener un ndarray float\n",
        "            Xv = np.asarray(X, dtype=float)\n",
        "\n",
        "        # Aplico min-max por columnas: (X - min) / range_safe_\n",
        "        X_scaled = (Xv - self.data_min_) / self.range_safe_\n",
        "\n",
        "        # Si el input fue DataFrame, devuelvo un DataFrame con mismos nombres/√≠ndice\n",
        "        if is_df:\n",
        "            return pd.DataFrame(X_scaled, columns=cols, index=idx)\n",
        "        else:\n",
        "            # Si fue ndarray, devuelvo ndarray\n",
        "            return X_scaled\n",
        "\n",
        "    def set_output(self, transform='default'):\n",
        "        # No modifico este m√©todo (seg√∫n enunciado); retorno self para encadenar\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySqWq1Muzp8"
      },
      "source": [
        "#### 5.2 Incorporando MinMax al pipeline [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIqjkgDwRsV"
      },
      "source": [
        "Ahora, usted decide agregar el escalamiento al pipeline, para lo que decide seguir los siguientes pasos:\n",
        "\n",
        "- Agregar el paso `minmax` al pipeline `numeric_transformations`, haciendo uso de la clase creada. [0.1 puntos]\n",
        "- Defina el dataframe `df_minmax` aplicando el ColumnTransformer actualizado a los datos proporcionados por Mr. Cheems. [0.1 puntos]\n",
        "- Usar `explore_data` en `df_retail` y en `df_minmax`. [0.1 puntos]\n",
        "- Reportar los cambios observados en la distribuci√≥n de las variables.  [0.2 puntos]\n",
        "\n",
        "**Nota:** Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a480355952a34b6cb7e72afa764091d6",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "lL2_CyAGXMfp",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ESTA CELDA TARDA ENTRE 25 A 30 MINUTOS EN CORRER SEG√öN COMPUTADOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "1889976b7a4c40c7825752979b577567",
        "deepnote_cell_height": 65.86666870117188,
        "deepnote_cell_type": "code",
        "id": "NmApXgB8XMfp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Considero pipeline y columntransformer que ya salio antes como el mejor -> el de knnimputer\n",
        "\n",
        "# redefino el pipeline num√©rico para agregar el paso MinMax\n",
        "numeric_transformations_knn = Pipeline(steps=[(\"iqr_transformer\", IQR(cte=1.5)), (\"knn_imputer\", KNNImputer(n_neighbors=2, weights=\"uniform\", metric=\"nan_euclidean\")), (\"minmax\", MinMax()),]) # aqu√≠ entonces sumo los pasos de \"recorte\" de outputs, \n",
        "                                                                                                                                                                                                # imputo con knn y suavizo a [0 y 1] con minmax este ultimo paso\n",
        "\n",
        "# redefino ColumnTransformer basado en mi ct_knn anterior, pero ahora con el pipeline que incluye MinMax\n",
        "\n",
        "ct_knn_minmax = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('categoric_transformations', categoric_transformations, categoric_cols),  # categ√≥ricas igual que antes\n",
        "        ('numeric_transformations_knn', numeric_transformations_knn, numeric_cols) # num√©ricas ahora incluyen MinMax o sea que estan en 0 y 1\n",
        "    ],\n",
        "    verbose_feature_names_out=False,   # mantengo nombres de col\n",
        "    remainder='passthrough'            # dejo pasar lo que no transformo para que no haya problema\n",
        ").set_output(transform='pandas')        # quiero que salga un DataFrame pq original parece que es array o algo asi\n",
        "\n",
        "# Aplico el ColumnTransformer actualizado a los datos.\n",
        "df_minmax = ct_knn_minmax.fit_transform(df_retail)  # ajusto y transformo sobre df_retail\n",
        "df_minmax = df_minmax[df_retail.columns]            # reordeno columnas como el original porque podr√≠an cambiar el orden\n",
        "\n",
        "# Exploro antes vs despu√©s para ver qu√© cambi√≥ con el escalado.\n",
        "print(\"EXPLORACI√ìN: df_retail (original)\")\n",
        "explore_data(df_retail, \"df_retail\")\n",
        "\n",
        "print(\"EXPLORACI√ìN: df_minmax (IQR + KNN + MinMax)\")\n",
        "explore_data(df_minmax, \"df_minmax\")\n",
        "\n",
        "# Resumen num√©rico para verificar que min‚âà0 y max‚âà1 en num√©ricas.\n",
        "print(\"\\nResumen r√°pido num√©ricos (min/max):\")\n",
        "print(\"Original (min/max):\")\n",
        "print(df_retail[numeric_cols].agg(['min','max']).T.head())\n",
        "print(\"\\nMinMax (min/max):\")\n",
        "print(df_minmax[numeric_cols].agg(['min','max']).T.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RESPUESTA VARIACIONES**\n",
        "\n",
        "Cuando comparamos las distribuciones originales (df_retail) con las transformadas (df_minmax) se notan cambios bien marcados. En el dataset original las variables Price y Quantity tienen escalas gigantes (Price llega a m√°s de 10.000 y Quantity sobre 19.000) y ambas se ven ultra cargadas a la izquierda con colas enormes hacia la derecha, lo que hace que casi toda la masa de datos se aplaste en los primeros bins. Adem√°s, hab√≠a nulos en esas columnas (Quantity ~7.900 y Price ~8.300).\n",
        "\n",
        "Despu√©s de aplicar el pipeline (IQR para cortar outliers, KNN para imputar y MinMax para escalar), las distribuciones cambian harto. Primero, los nulos desaparecen porque ya fueron imputados, y segundo, el rango de valores queda totalmente normalizado entre 0 y 1. Esto se nota en los histogramas de df_minmax, donde Price y Quantity se ven mucho m√°s ‚Äúextendidas‚Äù en el eje x y con colas mucho m√°s controladas. Ya no aparecen esos valores extremos que distorsionaban la escala, porque fueron tratados como outliers antes de escalar.\n",
        "\n",
        "Entonces la data original mostraba escalas desbalanceadas y con colas largas imposibles de leer, mientras que la versi√≥n escalada con MinMax entrega distribuciones m√°s parejas y comparables entre s√≠, sin nulos y sin distorsi√≥n por outliers extremos. Es decir, pasamos de un dataset desordenado y con magnitudes muy distintas a uno m√°s homog√©neo y m√°s amigable visualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXlAO8-wfNt"
      },
      "source": [
        "### 6. Pregunta te√≥rica [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvsFRwpVtMh_"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://file.coinexstatic.com/2023-09-19/166BAC031F222E5910954E7D7D0BC844.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou7lQIAHwiZv"
      },
      "source": [
        "Finalmente, expl√≠quele a Mr. Cheems porqu√© es √∫til la creaci√≥n de pipelines al momento de hacer Feature Engineering en Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29QJyzOCwjdD"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMDYYL1stUVO"
      },
      "source": [
        "Bueno Mr. Cheems, lo que ocurre es que cuando hablamos de *Feature Engineering* nos referimos al proceso de tomar los datos en bruto y transformarlos en variables que sean m√°s claras, √∫tiles y representativas para que un modelo de *Machine Learning* pueda aprender mejor los patrones. La verdad es que no importa tanto si el modelo es complejo o simple, lo que marca la diferencia es la calidad de las variables. O sea, un modelo sencillo, pero con buenas variables puede funcionar mucho mejor que uno complicado con variables mal hechas.\n",
        "\n",
        "Por lo mismo, aparecen los *pipelines*, que son s√∫per importantes porque permiten ordenar todos los pasos de transformaci√≥n de los datos antes de entrenar el modelo. En vez de ir haciendo cada cosa a mano una y otra vez, el pipeline asegura que procesos como rellenar valores faltantes, escalar, reducir dimensiones o seleccionar variables se hagan siempre en el mismo orden y de la misma manera. Eso ayuda a evitar errores y da m√°s seguridad en lo que estamos haciendo.\n",
        "\n",
        "Gracias a los pipelines podemos tener *reproducibilidad* en el c√≥digo porque cada vez que se corre, los pasos se repiten exactamente igual, sin sorpresas. Tambi√©n ayuda a evitar el *data leakage*, ya que las transformaciones se ajustan solo con los datos de entrenamiento y despu√©s se aplican al conjunto de prueba o validaci√≥n, respetando la separaci√≥n entre ellos (o sea, los conjuntos no se ‚Äúconocen‚Äù de antes).\n",
        "\n",
        "Otro punto importante, e igual derivado de lo que explicamos antes, es que los pipelines hacen el trabajo mucho m√°s *r√°pido y autom√°tico* porque basta con llamarlos una vez y todos los pasos se ejecutan sin tener que repetir cada transformaci√≥n manualmente. Esto adem√°s hace que sea m√°s *f√°cil probar cambios*, porque si quiero reemplazar un paso, lo cambio en el pipeline y se aplica en todo el flujo, sin tener que ir corrigiendo celda por celda.\n",
        "\n",
        "Y, por √∫ltimo, los pipelines tambi√©n funcionan muy bien con cosas como la *validaci√≥n cruzada* y la *b√∫squeda de hiperpar√°metros*. Esto b√°sicamente significa que, cuando queremos probar distintas configuraciones de un modelo (por ejemplo, distintos par√°metros o combinaciones de transformaciones), el pipeline se encarga de que todo el proceso se ejecute de manera ordenada desde el principio hasta el final. As√≠, no solo se entrena el modelo, sino que tambi√©n se aplican siempre las mismas transformaciones a los datos en cada prueba, lo que asegura que el preprocesamiento y el modelado est√©n realmente conectados y trabajen como un todo.\n",
        "\n",
        "Entonces, a modo de resumen, los pipelines son s√∫per √∫tiles porque hacen que todo el proceso de preparar los datos y entrenar modelos sea m√°s ordenado, seguro y eficiente. Ayudan a evitar errores, facilitan la experimentaci√≥n y aseguran que todo el trabajo que hacemos con los datos se mantenga consistente y reproducible, as√≠ que es clave usarlo ademas de ser una buena pr√°ctica en ciencia de datos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wrG4gYabzAHs",
        "MhISwri4zAHy",
        "QDwIXTh7bK_A",
        "Q6nm_0uWvrFv",
        "F4ZY_N0Ad1GP",
        "ECqH4t-Jvj05",
        "Pse94ohOm1um",
        "MF5s4dqMYCbJ",
        "buuUiW-9YYZ3",
        "1ddL8wThv36t",
        "qOV0y-e_lS39",
        "4dk2R1kvuu-e",
        "RySqWq1Muzp8",
        "iWXlAO8-wfNt"
      ],
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "33c253a4f84d40a091bd5023e95abb64",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
