{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac9155b9f5e04400957a6f8bb3f6610c",
        "deepnote_cell_type": "markdown",
        "id": "2v2D1coL7I8i"
      },
      "source": [
        "<h1><center>Laboratorio 4: La solicitud de Mathias ü§ó</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d3d6f6d405c54dbe985a5f4b3e4f9120",
        "deepnote_cell_type": "markdown",
        "id": "YxdTmIPD7L_x"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "851a7788e8214942863cbd4099064ab2",
        "deepnote_cell_type": "markdown",
        "id": "Y2Gyrj-x7N2L"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Josefa Anselmo\n",
        "- Nombre de alumno 2: Tamara Carrasco \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f23a189afdec4e198683308db70e43b7",
        "deepnote_cell_type": "markdown",
        "id": "jQ9skYc57Pxi"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [TamoJosha](https://github.com/Tamaracarrasco/TamoJosha-Lab-de-Prog-MDS-Prim2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5318f41cda64d4290a7a548956ed725",
        "deepnote_cell_type": "markdown",
        "id": "1M4PoEWm7S80"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers.\n",
        "- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.\n",
        "- Familiarizarse con plotly.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "858df483d9e64780a21674afed1d34b8",
        "deepnote_cell_type": "markdown",
        "id": "SuMbiyQZG2Cc"
      },
      "source": [
        "## Descripci√≥n del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "403ffe48ec994afda4b91e670a08d0ef",
        "deepnote_cell_type": "markdown",
        "id": "QZsNO4rUrqCz"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0303baa17d4546feae8c9b88c58470bf",
        "deepnote_cell_type": "markdown",
        "id": "2o0MPuk8rqCz"
      },
      "source": [
        "En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Mathias, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Mathias, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.\n",
        "\n",
        "Mathias encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.\n",
        "\n",
        "A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e78cb41b144041af98928ab26dcfdaa9",
        "deepnote_cell_type": "markdown",
        "id": "hs4KKWF1Hdpo"
      },
      "source": [
        "## Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "95a5533cfd6d49cfb9afc111c44d224f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1714107106552,
        "id": "a4YpMafirqC0",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importamos esto para el renderizado de vscode\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "acbeab32db6146678e75448dddf43da8",
        "deepnote_cell_type": "markdown",
        "id": "UQOXod4gHhSq"
      },
      "source": [
        "## 1. Estudio de Performance üìà [10 Puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "704b56b978254ad3ae12cdbf58f4832d",
        "deepnote_cell_type": "markdown",
        "id": "Gn5u5ICkrqC2"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/23/b7/6e/23b76e9e77e63c0eec1a7b28372369e3.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d35fbdcc5ef045d6a2822622f0714179",
        "deepnote_cell_type": "markdown",
        "id": "y4Z0jTjtrqC2"
      },
      "source": [
        "Don Mathias les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.\n",
        "\n",
        "Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.\n",
        "\n",
        "Uno de los requisitos establecidos por Mathias es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />\n",
        "\n",
        "En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37580aab6cef4238a8ce42c50a6d35de",
        "deepnote_cell_type": "markdown",
        "id": "maCUNAvZrqC2"
      },
      "source": [
        "Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:\n",
        "1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]\n",
        "2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]\n",
        "3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]\n",
        "\n",
        "\n",
        "> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.\n",
        "\n",
        "> ‚ùó Recuerde que es obligatorio el uso de plotly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7f7c25e366754595b13fc2e8116f65a0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1714107108441,
        "id": "i0IZPGPOrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 1000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y51s6f_UtIkc"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kneed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importaci√≥n de librer√≠as para la fnci√≥n plot_scatter\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.cluster.hierarchy import  ward\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture \n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# para obtener eps optimo\n",
        "import sys\n",
        "!{sys.executable} -m pip install kneed\n",
        "\n",
        "from kneed import KneeLocator\n",
        "\n",
        "# m√©trica de silhouette\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# librer√≠a para el tiempo de ejecuci√≥n\n",
        "\n",
        "import time\n",
        "\n",
        "# EN CASO DE QUE NO FUNCIONE PLOTLY:\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_sets[\"moons\"][\"x\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Para el m√©todo de DBSCAN:\n",
        "# los hiperpar√°metros de eps se espera que uno escoja un valor √≥ptimo de acuerdo a la \n",
        "#distancia del N-esimo vecino m√°s cercano y usar el m√©todo del codo.\n",
        "# MIN_pts = 4, es un valor sugerido de la clase de Machine Learning\n",
        "\n",
        "# propongo hacer un m√©todo para obtener un eps sugerido\n",
        "\n",
        "def eps_dbscan(data, k=10, plot=\"no\"):\n",
        "    \"\"\"Esta funci√≥n recibe el diccionario y\n",
        "    devolver√° un array con 3 posibles valores de eps\n",
        "    para cada forma de cluster\n",
        "    \n",
        "    Par√°metros:\n",
        "    ---------------\n",
        "    data: diccionario data_sets\n",
        "    \n",
        "    k = (int) n√∫mero de vecinos cercanos. \n",
        "    Por default k=10 para los tres tipos de cluster.\n",
        "\n",
        "    plot: (string) Default \"no\", si es \"yes\" hay que \n",
        "    arreglar esa parte pero se tendr√≠a un gr√°fico donde\n",
        "    el eje y ser√≠a la distancia epsilon (eps) vs\n",
        "    el n√∫mero de puntos y se marcar√≠a la distancia eps √≥ptima.\n",
        "    \n",
        "    Returns:\n",
        "    --------------\n",
        "    por ahora printeos con los eps √≥ptimos \"\"\"\n",
        "\n",
        "    ### Moons\n",
        "    \n",
        "    neighbors_moons = NearestNeighbors(n_neighbors = k) \n",
        "    neighbors_fit_moons = neighbors_moons.fit(data.get(\"moons\").get(\"x\"))\n",
        "    distances_moons, indices_moons = neighbors_fit_moons.kneighbors(data.get(\"moons\").get(\"x\"))\n",
        "\n",
        "    distances_moons = np.sort(distances_moons[:, 9])\n",
        "\n",
        "    # Detecci√≥n autom√°tica del codo\n",
        "    kneedle_moons = KneeLocator(range(len(distances_moons)), distances_moons, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
        "\n",
        "    eps_sugerido_moons = distances_moons[kneedle_moons.knee]\n",
        "\n",
        "    ### BLOBS\n",
        "\n",
        "    neighbors_blobs = NearestNeighbors(n_neighbors = k) \n",
        "    neighbors_fit_blobs = neighbors_blobs.fit(data.get(\"blobs\").get(\"x\"))\n",
        "    distances_blobs, indices_blobs = neighbors_fit_blobs.kneighbors(data.get(\"blobs\").get(\"x\"))\n",
        "\n",
        "    distances_blobs = np.sort(distances_blobs[:, 9])\n",
        "\n",
        "    # Detecci√≥n autom√°tica del codo\n",
        "    kneedle_blobs = KneeLocator(range(len(distances_blobs)), distances_blobs, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
        "\n",
        "    eps_sugerido_blobs = distances_blobs[kneedle_blobs.knee]\n",
        "\n",
        "    ### MUTATED\n",
        "    neighbors_mut = NearestNeighbors(n_neighbors = k) \n",
        "    neighbors_fit_mut = neighbors_mut.fit(data.get(\"mutated\").get(\"x\"))\n",
        "    distances_mut, indices_mut = neighbors_fit_mut.kneighbors(data.get(\"mutated\").get(\"x\"))\n",
        "\n",
        "    distances_mut = np.sort(distances_mut[:, 9])\n",
        "\n",
        "    # Detecci√≥n autom√°tica del codo\n",
        "    kneedle_mut = KneeLocator(range(len(distances_mut)), distances_mut, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
        "\n",
        "    eps_sugerido_mut = distances_mut[kneedle_mut.knee]\n",
        "\n",
        "    if plot==\"yes\":\n",
        "\n",
        "        # plt.figure(figsize=(8, 4))\n",
        "        # plt.plot(distances)\n",
        "        # plt.title(\"Gr√°fico de distancia al con k=10 vecinos m√°s cercanos\")\n",
        "        # plt.xlabel(\"Puntos ordenados\")\n",
        "        # plt.ylabel(r\"Distancia $\\varepsilon$\")\n",
        "        # plt.grid(True)\n",
        "        # plt.show()\n",
        "\n",
        "        # # Grafico con el codo marcado\n",
        "        # plt.figure(figsize=(8, 5))\n",
        "        # plt.plot(distances, label=\"k-distancias\")\n",
        "        # plt.axvline(kneedle.knee, color='red', linestyle='--', label=\"Codo detectado\")\n",
        "        # plt.axhline(eps_sugerido_moons, color='green', linestyle='--', label=f\"eps ‚âà {eps_sugerido_moons:.4f}\")\n",
        "        # plt.xlabel(\"Puntos ordenados\")\n",
        "        # plt.ylabel(r\"Distancia $\\varepsilon$ al 10¬∞ vecino m√°s cercano\")\n",
        "        # plt.title(f\"Detecci√≥n autom√°tica del codo (k-distancias) para {data.keys()[0]}\")\n",
        "        # plt.legend()\n",
        "        # plt.grid(True)\n",
        "        # plt.show()\n",
        "        pass\n",
        "    \n",
        "    else:\n",
        "        print(f\"eps sugerido autom√°ticamente con k = 10 vecinos + cercanos (moons): {eps_sugerido_moons:.4f}\")\n",
        "        print(f\"eps sugerido autom√°ticamente con k = 10 vecinos + cercanos (blobs): {eps_sugerido_blobs:.4f}\")\n",
        "        print(f\"eps sugerido autom√°ticamente k = 10 vecinos + cercanos (mut): {eps_sugerido_mut:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eps_dbscan(data_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "643d6b35af5541358f481fda4d3fc51f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 267,
        "execution_start": 1714108733824,
        "id": "CO3JFqezrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "def plot_scatter(x, y=None, color=None):\n",
        "\n",
        "    \"\"\"Funci√≥n que entrega plots comparativos\n",
        "    de tiempos de ejecuci√≥n para diferentes t√©cnicas de clustering.\n",
        "    \n",
        "    Par√°metros iniciales:\n",
        "    ------------------------\n",
        "    x: es un diccionario.\n",
        "    \n",
        "    y: default None. No me queda claro como usarlo\n",
        "    \n",
        "    color: None, tampoco me queda claro como usarlo.\"\"\"\n",
        "\n",
        "    ###------ MOONS------ ###\n",
        "\n",
        "    X_moons = x.get(\"moons\").get(\"x\")\n",
        "\n",
        "    #### Kmeans #####\n",
        "\n",
        "    start = time.time() # inicio del tiempo de ejecuci√≥n\n",
        "\n",
        "    kmeans_moons = KMeans(n_clusters=x.get(\"moons\").get(\"n_cluster\"), random_state=42) # modelo kmeans\n",
        "    y_pred_km_moons = kmeans_moons.fit_predict(X_moons) # predicciones\n",
        "    t_km_moons = time.time() - start # delta t del tiempo de ejecuci√≥n.\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_km_moons = kmeans_moons.labels_ \n",
        "    sil_km_moons = silhouette_score(X_moons, labels_km_moons)\n",
        "\n",
        "    #####  GMM ######\n",
        "\n",
        "    start = time.time()\n",
        "    gmm_moons = GaussianMixture(n_components=x.get(\"moons\").get(\"n_cluster\"), random_state=42)\n",
        "    gmm_moons.fit(X_moons) ### GMM no tiene .labels_ para calcular el coeficiente, por lo que no se puede usar ._fit_predict()\n",
        "                            #### sino que fit y predict por separado.\n",
        "    y_pred_gmm_moons = gmm_moons.predict(X_moons)\n",
        "    t_gmm_moons = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_gmm_moons = y_pred_gmm_moons\n",
        "    sil_gmm_moons = silhouette_score(X_moons, labels_gmm_moons)\n",
        "\n",
        "    ####  WARD #######\n",
        "    start = time.time()\n",
        "    ward_moons = AgglomerativeClustering(n_clusters=x.get(\"moons\").get(\"n_cluster\"), linkage=\"ward\")\n",
        "    y_pred_ward_moons = ward_moons.fit_predict(X_moons)\n",
        "    t_ward_moons = time.time() - start\n",
        "\n",
        "     # para el coeficiente de Sil\n",
        "    labels_ward_moons = ward_moons.labels_ \n",
        "    sil_ward_moons = silhouette_score(X_moons, labels_ward_moons)\n",
        "\n",
        "    #####  DBSCAN ######\n",
        "\n",
        "    start = time.time()\n",
        "    dbscan_moons = DBSCAN(eps=0.1, min_samples=10)\n",
        "    y_pred_dbscan_moons = dbscan_moons.fit_predict(X_moons)\n",
        "    t_dbscan_moons = time.time() - start\n",
        "    \n",
        "    # para el coeficiente de Sil\n",
        "    labels_dbscan_moons = dbscan_moons.labels_ \n",
        "    sil_dbscan_moons = silhouette_score(X_moons, labels_dbscan_moons)\n",
        "\n",
        "    ###------BLOBS------ ###\n",
        "    \n",
        "    X_blobs = x.get(\"blobs\").get(\"x\")\n",
        "\n",
        "    #### KMeans ####\n",
        "    start = time.time()\n",
        "    kmeans_blobs = KMeans(n_clusters=x.get(\"blobs\").get(\"n_cluster\"), random_state=42)\n",
        "    y_pred_km_blobs = kmeans_blobs.fit_predict(X_blobs)\n",
        "    t_km_blobs = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_km_blobs = kmeans_blobs.labels_ \n",
        "    sil_km_blobs = silhouette_score(X_blobs, labels_km_blobs)\n",
        "\n",
        "    #####  GMM ######\n",
        "\n",
        "    start = time.time()\n",
        "    gmm_blobs = GaussianMixture(n_components=x.get(\"blobs\").get(\"n_cluster\"), random_state=42)\n",
        "    gmm_blobs.fit(X_blobs) ### GMM no tiene .labels_ para calcular el coeficiente, por lo que no se puede usar ._fit_predict()\n",
        "                            #### sino que fit y predict por separado.\n",
        "    y_pred_gmm_blobs = gmm_blobs.predict(X_blobs)\n",
        "    t_gmm_blobs = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_gmm_blobs = y_pred_gmm_blobs\n",
        "    sil_gmm_blobs = silhouette_score(X_blobs, labels_gmm_blobs)\n",
        "\n",
        "    #### WARD #####\n",
        "\n",
        "    start = time.time()\n",
        "    ward_blobs = AgglomerativeClustering(n_clusters=x.get(\"blobs\").get(\"n_cluster\"), linkage=\"ward\")\n",
        "    y_pred_ward_blobs = ward_blobs.fit_predict(X_blobs)\n",
        "    t_ward_blobs = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_ward_blobs = ward_blobs.labels_ \n",
        "    sil_ward_blobs = silhouette_score(X_blobs, labels_ward_blobs)\n",
        "\n",
        "    ### DBSCAN ####\n",
        "\n",
        "    start = time.time()\n",
        "    dbscan_blobs = DBSCAN(eps=0.7, min_samples=4)\n",
        "    y_pred_dbscan_blobs = dbscan_blobs.fit_predict(X_blobs)\n",
        "    t_dbscan_blobs = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_dbscan_blobs = dbscan_blobs.labels_ \n",
        "    sil_dbscan_blobs = silhouette_score(X_blobs, labels_dbscan_blobs)\n",
        "\n",
        "    ###-------- MUTATED -------- ###\n",
        "\n",
        "    X_mut = x.get(\"mutated\").get(\"x\")\n",
        "\n",
        "    #### KMeans ####\n",
        "    start = time.time()\n",
        "    kmeans_mut = KMeans(n_clusters=x.get(\"mutated\").get(\"n_cluster\"), random_state=42)\n",
        "    y_pred_km_mut = kmeans_mut.fit_predict(X_mut)\n",
        "    t_km_mut = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_km_mut = kmeans_mut.labels_ \n",
        "    sil_km_mut = silhouette_score(X_mut, labels_km_mut)\n",
        "\n",
        "    #### GMM ####\n",
        "\n",
        "    start = time.time()\n",
        "    gmm_mut = GaussianMixture(n_components=x.get(\"mutated\").get(\"n_cluster\"), random_state=42)\n",
        "    gmm_mut.fit(X_mut) ### GMM no tiene .labels_ para calcular el coeficiente, por lo que no se puede usar ._fit_predict()\n",
        "                            #### sino que fit y predict por separado.\n",
        "    y_pred_gmm_mut = gmm_mut.predict(X_mut)\n",
        "    t_gmm_mut = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_gmm_mut = y_pred_gmm_mut\n",
        "    sil_gmm_mut = silhouette_score(X_mut, labels_gmm_mut)\n",
        "\n",
        "    ### WARD ####\n",
        "\n",
        "    start = time.time()\n",
        "    ward_mut = AgglomerativeClustering(n_clusters=x.get(\"mutated\").get(\"n_cluster\"), linkage=\"ward\")\n",
        "    y_pred_ward_mut = ward_mut.fit_predict(X_mut)\n",
        "    t_ward_mut = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_ward_mut = ward_mut.labels_ \n",
        "    sil_ward_mut = silhouette_score(X_mut, labels_ward_mut)\n",
        "\n",
        "    #### DBSCAN #####\n",
        "\n",
        "    start = time.time()\n",
        "    dbscan_mut = DBSCAN(eps=0.5, min_samples=3)\n",
        "    y_pred_dbscan_mut = dbscan_mut.fit_predict(X_mut)\n",
        "    t_dbscan_mut = time.time() - start\n",
        "\n",
        "    # para el coeficiente de Sil\n",
        "    labels_dbscan_mut = dbscan_mut.labels_ \n",
        "    sil_dbscan_mut = silhouette_score(X_mut, labels_dbscan_mut)\n",
        "\n",
        "    ### Figura 3x4\n",
        "    fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(9, 7))\n",
        "    fig.suptitle(\"Comparaci√≥n de tiempos de ejecuci√≥n por t√©cnica\", fontsize=16, y=1)\n",
        "\n",
        "    ### MOONS \n",
        "\n",
        "    axs[0, 0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred_km_moons)\n",
        "    axs[0, 0].set_title(f\"KMEANS\\n{t_km_moons:.2f} [s] | s: {sil_km_moons:.1f} \")\n",
        "\n",
        "    axs[0, 1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred_gmm_moons)\n",
        "    axs[0, 1].set_title(f\"GMM\\n{t_gmm_moons:.2f} [s] | s: {sil_gmm_moons:.1f}\")\n",
        "\n",
        "    axs[0, 2].scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred_ward_moons)\n",
        "    axs[0, 2].set_title(f\"WARD\\n{t_ward_moons:.2f} [s] | s: {sil_ward_moons:.1f}\")\n",
        "\n",
        "    axs[0, 3].scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred_dbscan_moons)\n",
        "    axs[0, 3].set_title(f\"DBSCAN\\n{t_dbscan_moons:.2f} [s] | s: {sil_dbscan_moons:.1f}\")\n",
        "\n",
        "    ### BLOBS\n",
        "    axs[1, 0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_pred_km_blobs)\n",
        "    axs[1, 0].set_title(f\"\\n{t_km_blobs:.2f} [s] | s: {sil_km_blobs:.1f} \")\n",
        "\n",
        "    axs[1, 1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_pred_gmm_blobs)\n",
        "    axs[1, 1].set_title(f\"\\n{t_gmm_blobs:.2f} [s] | s: {sil_gmm_blobs:.1f}\")\n",
        "\n",
        "    axs[1, 2].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_pred_ward_blobs)\n",
        "    axs[1, 2].set_title(f\"\\n{t_ward_blobs:.2f} [s] | s: {sil_ward_blobs:.1f}\")\n",
        "\n",
        "    axs[1, 3].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_pred_dbscan_blobs)\n",
        "    axs[1, 3].set_title(f\"\\n{t_dbscan_blobs:.2f} [s] | s: {sil_dbscan_blobs:.1f}\")\n",
        "\n",
        "    ### MUTATED\n",
        "    axs[2, 0].scatter(X_mut[:, 0], X_mut[:, 1], c=y_pred_km_mut)\n",
        "    axs[2, 0].set_title(f\"\\n{t_km_mut:.2f} [s] | s: {sil_km_mut:.1f}\")\n",
        "\n",
        "    axs[2, 1].scatter(X_mut[:, 0], X_mut[:, 1], c=y_pred_gmm_mut)\n",
        "    axs[2, 1].set_title(f\"\\n{t_gmm_mut:.2f} [s] | s: {sil_gmm_mut:.1f}\")\n",
        "\n",
        "    axs[2, 2].scatter(X_mut[:, 0], X_mut[:, 1], c=y_pred_ward_mut)\n",
        "    axs[2, 2].set_title(f\"\\n{t_ward_mut:.2f} [s] | s: {sil_ward_mut:.1f}\")\n",
        "\n",
        "    axs[2, 3].scatter(X_mut[:, 0], X_mut[:, 1], c=y_pred_dbscan_mut)\n",
        "    axs[2, 3].set_title(f\"\\n{t_dbscan_mut:.2f} [s] | s: {sil_dbscan_mut:.1f}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### En la celda de abajo, est√° la versi√≥n de plot_scatter usando PLOTLY\n",
        "Solo hay que desilenciar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### versi√≥n de plot_scatter usando PLOTLY\n",
        "# por ahora quedar√° silenciado\n",
        "\n",
        "# def plot_scatter(x, y=None, color=None):\n",
        "\n",
        "#      \"\"\"Funci√≥n que entrega plots comparativos\n",
        "#     de tiempos de ejecuci√≥n para diferentes t√©cnicas de clustering.\n",
        "    \n",
        "#     Par√°metros iniciales:\n",
        "#     ------------------------\n",
        "#     x: es un dccionario.\n",
        "    \n",
        "#     y: default None. No me queda claro como usarlo\n",
        "    \n",
        "#     color: None, tampoco me queda claro como usarlo.\"\"\"\n",
        "\n",
        "#     ##### ===== MOONS ======= #####\n",
        "#     X_moons = x.get(\"moons\").get(\"x\")\n",
        "\n",
        "#     #### KMEANS ####\n",
        "\n",
        "#     start = time.time()\n",
        "#     kmeans_moons = KMeans(n_clusters=x.get(\"moons\").get(\"n_cluster\"), random_state=42)\n",
        "#     y_pred_km_moons = kmeans_moons.fit_predict(X_moons)\n",
        "#     t_km_moons = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_km_moons = silhouette_score(X_moons, y_pred_km_moons)\n",
        "\n",
        "#     #### GMM ####\n",
        "#     start = time.time()\n",
        "#     gmm_moons = GaussianMixture(n_components=x.get(\"moons\").get(\"n_cluster\"), random_state=42) ### GMM no tiene .labels_ para calcular el coeficiente, por lo que no se puede usar ._fit_predict()\n",
        "#                                                                                             #### sino que fit y predict por separado.\n",
        "#     gmm_moons.fit(X_moons)\n",
        "#     y_pred_gmm_moons = gmm_moons.predict(X_moons)\n",
        "#     t_gmm_moons = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_gmm_moons = silhouette_score(X_moons, y_pred_gmm_moons)\n",
        "\n",
        "#     #### WARD ####\n",
        "\n",
        "#     start = time.time()\n",
        "#     ward_moons = AgglomerativeClustering(n_clusters=x.get(\"moons\").get(\"n_cluster\"), linkage=\"ward\")\n",
        "#     y_pred_ward_moons = ward_moons.fit_predict(X_moons)\n",
        "#     t_ward_moons = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_ward_moons = silhouette_score(X_moons, y_pred_ward_moons)\n",
        "\n",
        "#     #### DBSCAN ####\n",
        "\n",
        "#     start = time.time()\n",
        "#     dbscan_moons = DBSCAN(eps=0.1, min_samples=4)\n",
        "#     y_pred_dbscan_moons = dbscan_moons.fit_predict(X_moons)\n",
        "#     t_dbscan_moons = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_dbscan_moons = silhouette_score(X_moons, y_pred_dbscan_moons)\n",
        "\n",
        "#     #### ======= BLOBS ========= #####\n",
        "\n",
        "#     X_blobs = x.get(\"blobs\").get(\"x\")\n",
        "\n",
        "#     #### KMEANS #####\n",
        "\n",
        "#     start = time.time()\n",
        "#     kmeans_blobs = KMeans(n_clusters=x.get(\"blobs\").get(\"n_cluster\"), random_state=42)\n",
        "#     y_pred_km_blobs = kmeans_blobs.fit_predict(X_blobs)\n",
        "#     t_km_blobs = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_km_blobs = silhouette_score(X_blobs, y_pred_km_blobs)\n",
        "\n",
        "#     #### GMM ####\n",
        "#     start = time.time()\n",
        "#     gmm_blobs = GaussianMixture(n_components=x.get(\"blobs\").get(\"n_cluster\"), random_state=42)\n",
        "#     gmm_blobs.fit(X_blobs)\n",
        "#     y_pred_gmm_blobs = gmm_blobs.predict(X_blobs)\n",
        "#     t_gmm_blobs = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_gmm_blobs = silhouette_score(X_blobs, y_pred_gmm_blobs)\n",
        "\n",
        "#     #### WARD ####\n",
        "#     start = time.time()\n",
        "#     ward_blobs = AgglomerativeClustering(n_clusters=x.get(\"blobs\").get(\"n_cluster\"), linkage=\"ward\")\n",
        "#     y_pred_ward_blobs = ward_blobs.fit_predict(X_blobs)\n",
        "#     t_ward_blobs = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_ward_blobs = silhouette_score(X_blobs, y_pred_ward_blobs)\n",
        "\n",
        "#     #### DBSCAN ####\n",
        "#     start = time.time()\n",
        "#     dbscan_blobs = DBSCAN(eps=0.5, min_samples=4)\n",
        "#     y_pred_dbscan_blobs = dbscan_blobs.fit_predict(X_blobs)\n",
        "#     t_dbscan_blobs = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_dbscan_blobs = silhouette_score(X_blobs, y_pred_dbscan_blobs)\n",
        "\n",
        "#     #### ====== MUTATED ====== ####\n",
        "#     X_mut = x.get(\"mutated\").get(\"x\")\n",
        "\n",
        "#     #### KMEANS ####\n",
        "#     start = time.time()\n",
        "#     kmeans_mut = KMeans(n_clusters=x.get(\"mutated\").get(\"n_cluster\"), random_state=42)\n",
        "#     y_pred_km_mut = kmeans_mut.fit_predict(X_mut)\n",
        "#     t_km_mut = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_km_mut = silhouette_score(X_mut, y_pred_km_mut)\n",
        "\n",
        "#     #### GMM ####\n",
        "#     start = time.time()\n",
        "#     gmm_mut = GaussianMixture(n_components=x.get(\"mutated\").get(\"n_cluster\"), random_state=42)\n",
        "#     gmm_mut.fit(X_mut)\n",
        "#     y_pred_gmm_mut = gmm_mut.predict(X_mut)\n",
        "#     t_gmm_mut = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_gmm_mut = silhouette_score(X_mut, y_pred_gmm_mut)\n",
        "\n",
        "#     #### WARD ####\n",
        "#     start = time.time()\n",
        "#     ward_mut = AgglomerativeClustering(n_clusters=x.get(\"mutated\").get(\"n_cluster\"), linkage=\"ward\")\n",
        "#     y_pred_ward_mut = ward_mut.fit_predict(X_mut)\n",
        "#     t_ward_mut = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_ward_mut = silhouette_score(X_mut, y_pred_ward_mut)\n",
        "\n",
        "#     #### DBSCAN ###\n",
        "#     start = time.time()\n",
        "#     dbscan_mut = DBSCAN(eps=0.1, min_samples=4)\n",
        "#     y_pred_dbscan_mut = dbscan_mut.fit_predict(X_mut)\n",
        "#     t_dbscan_mut = time.time() - start\n",
        "#     # coef sil\n",
        "#     sil_dbscan_mut = silhouette_score(X_mut, y_pred_dbscan_mut)\n",
        "\n",
        "#     # Gr√°fico\n",
        "#     fig = make_subplots(rows=3, cols=4,\n",
        "#                         subplot_titles=[\n",
        "#                             f\"KMEANS <br>{t_km_moons:.2f} [s] | s: {sil_km_moons:.1f}\",\n",
        "#                             f\"GMM <br>{t_gmm_moons:.2f} [s] | s: {sil_gmm_moons:.1f}\",\n",
        "#                             f\"WARD <br>{t_ward_moons:.2f} [s] | s: {sil_ward_moons:.1f}\",\n",
        "#                             f\"DBSCAN <br>{t_dbscan_moons:.2f} [s] | s: {sil_dbscan_moons:.1f}\",\n",
        "\n",
        "#                             f\"{t_km_blobs:.2f} [s] | s: {sil_km_blobs:.1f}\",\n",
        "#                             f\"{t_gmm_blobs:.2f} [s] | s: {sil_gmm_blobs:.1f}\",\n",
        "#                             f\"{t_ward_blobs:.2f} [s] | s: {sil_ward_blobs:.1f}\",\n",
        "#                             f\"{t_dbscan_blobs:.2f} [s] | s: {sil_dbscan_blobs:.1f}\",\n",
        "\n",
        "#                             f\"{t_km_mut:.2f} [s] | s: {sil_km_mut:.1f}\",\n",
        "#                             f\"{t_gmm_mut:.2f} [s] | s: {sil_gmm_mut:.1f}\",\n",
        "#                             f\"{t_ward_mut:.2f} [s] | s: {sil_ward_mut:.1f}\",\n",
        "#                             f\"{t_dbscan_mut:.2f} [s] | s: {sil_dbscan_mut:.1f}\",])\n",
        "\n",
        "#     # MOONS\n",
        "#     fig.add_trace(go.Scatter(x=X_moons[:, 0], y=X_moons[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_km_moons, colorscale=\"Viridis\")),\n",
        "#                   row=1, col=1)\n",
        "#     fig.add_trace(go.Scatter(x=X_moons[:, 0], y=X_moons[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_gmm_moons, colorscale=\"Viridis\")),\n",
        "#                   row=1, col=2)\n",
        "#     fig.add_trace(go.Scatter(x=X_moons[:, 0], y=X_moons[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_ward_moons, colorscale=\"Viridis\")),\n",
        "#                   row=1, col=3)\n",
        "#     fig.add_trace(go.Scatter(x=X_moons[:, 0], y=X_moons[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_dbscan_moons, colorscale=\"Viridis\")),\n",
        "#                   row=1, col=4)\n",
        "\n",
        "#     # BLOBS\n",
        "#     fig.add_trace(go.Scatter(x=X_blobs[:, 0], y=X_blobs[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_km_blobs, colorscale=\"Viridis\")),\n",
        "#                   row=2, col=1)\n",
        "#     fig.add_trace(go.Scatter(x=X_blobs[:, 0], y=X_blobs[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_gmm_blobs, colorscale=\"Viridis\")),\n",
        "#                   row=2, col=2)\n",
        "#     fig.add_trace(go.Scatter(x=X_blobs[:, 0], y=X_blobs[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_ward_blobs, colorscale=\"Viridis\")),\n",
        "#                   row=2, col=3)\n",
        "#     fig.add_trace(go.Scatter(x=X_blobs[:, 0], y=X_blobs[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_dbscan_blobs, colorscale=\"Viridis\")),\n",
        "#                   row=2, col=4)\n",
        "\n",
        "#     # MUTATED\n",
        "#     fig.add_trace(go.Scatter(x=X_mut[:, 0], y=X_mut[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_km_mut, colorscale=\"Viridis\")),\n",
        "#                   row=3, col=1)\n",
        "#     fig.add_trace(go.Scatter(x=X_mut[:, 0], y=X_mut[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_gmm_mut, colorscale=\"Viridis\")),\n",
        "#                   row=3, col=2)\n",
        "#     fig.add_trace(go.Scatter(x=X_mut[:, 0], y=X_mut[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_ward_mut, colorscale=\"Viridis\")),\n",
        "#                   row=3, col=3)\n",
        "#     fig.add_trace(go.Scatter(x=X_mut[:, 0], y=X_mut[:, 1], mode=\"markers\",\n",
        "#                              marker=dict(color=y_pred_dbscan_mut, colorscale=\"Viridis\")),\n",
        "#                   row=3, col=4)\n",
        "\n",
        "#     fig.update_layout(height=900, width=1200,\n",
        "#                       title_text=\"Comparaci√≥n de tiempos de ejecuci√≥n por t√©cnica\")\n",
        "\n",
        "#     fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ac√° se plotea el data_sets de 1000 puntos.\n",
        "\n",
        "plot_scatter(data_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ac√° se plotea el datasets con 5000 puntos (cambi√© el n_samples de manera manual)\n",
        "# y tambi√©n copi√© la celda entera mejor porque estar cambiando de manera manual y \n",
        "# tener ojo con qu√© celda correr es agotador\n",
        "\n",
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 5_000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)\n",
        "\n",
        "plot_scatter(data_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ac√° se plotea el datasets de 10_000 puntos \n",
        "# se cambia manulamente en la celda de datasets y se corre ese bloque solamente\n",
        "\n",
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 10_000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)\n",
        "plot_scatter(data_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lisis de las t√©cnicas de clustering\n",
        "\n",
        "## Del sample de tama√±o 1000 puntos\n",
        "\n",
        "### Para los clusters tipo \"MOONS\"\n",
        "\n",
        "La mejor t√©cnica que pudo identificar los clusters de manera correcta fue DBSCAN, ya que este m√©todo es \n",
        "preferible si se sabe que la estructura de los clsuters no son esf√©ricas. No solo eso, tambi√©n este m√©todo\n",
        "result√≥ como el m√°s r√°pido con un tiempo de ejecuci√≥n de 0.01 [s] al momento de correr la celda.\n",
        "Llama la atensi√≥n que el coeficiente de Silhouette sea el m√°s bajo de las 4 t√©cnicas aunque igual tiene sentido\n",
        "ya que los clusters \"MOONS\" est√°n muy cerca de solaparse.\n",
        "\n",
        "\n",
        "Era esperable que Kmeans y WARD fallaran al identificar los clusters ya que: Kmeans tiene problemas cuando\n",
        "se trata de clusters no esfer√≠cos por la m√©trica euclidiana que se usa en su algoritmo y WARD\n",
        "(Por c√≥mo funciona su algoritmo de fusi√≥n) va a preferir particiones donde cada cluster tenga \n",
        "baja varianza en todas las dirreciones como clusters redondos y compactos.\n",
        "\n",
        "GMM tambi√©n fall√≥ en identificar los clusters porque uno de los supuestos de este algoritmo es que cada cluster \n",
        "es modelado como una gaussiana el√≠ptica; los datos provienen de uns mezcla de distribuciones normales multivariadas,\n",
        "cosa que clusters con la forma de luna no cumplen.\n",
        "\n",
        "En este caso de n_samples = 1000, KMEANS fue la t√©cnica que m√°s tard√≥ en tiempo de ejecuci√≥n, cosa que no\n",
        "sucede cuando aumenta n_samples a 5000 y a 10000\n",
        "\n",
        "\n",
        "### Para clusters tipo BLOBS\n",
        "\n",
        "todas las t√©cnicas pudieron identificar a los 3 clusters, siendo KMEANS  la t√©cnica con m√°s rapidez en el c√°lculo.\n",
        "De hecho, todas las t√©cnicas registran un alto coeficiente de Silhouette $\\sim 0.8$, lo cual es bueno, mientras m√°s cerca a 1 sea\n",
        "este coeficiente, m√°s compactos y definidos son los clusters y est√°n separados unos con los otros.\n",
        "\n",
        "Debido a los hiperpar√°metros escogidos para DBSCAN como min_pts=4, se observa mayor cantidad\n",
        "de puntos ruidos que en los clusters tipo \"moons\"\n",
        "\n",
        "### Para clusters tipo MUTATED.\n",
        "\n",
        "Nuevamente, este tipo de cluster no es esf√©rico, lo cual KMEANS y WARD no identifica los 3 tipo de cluster de manera\n",
        "correcta.DBSCAN solo identifica 2 tipos de clusters ya que por la elecci√≥n de los hiperpar√°metros (en particular eps), consider√≥\n",
        "los dos clusters ubicados sobre la recta Y = 0, como uno solo, ya que estos clusters est√°n tan juntos y el eps √≥ptimo encontrado\n",
        "considerando k = 10 es relativamente peque√±o (eps $\\sim 0.5$), los clusters son considerados como uno solo por el criterio de alcanzabilidad por densidad.\n",
        "\n",
        "El √∫nico m√©todo que pudo identificar bien la estructura fue GMM, ya que para este tipo de clusters\n",
        "si se podr√≠a cumplir la susposiciones de que los puntos presentan una distribuci√≥n gaussiana.\n",
        "\n",
        "Sobre al aumentar el tama√±o de la muetsra a 5000 y 10000, se aprecian las siguientes diferencias:\n",
        "## n_samples = 5000\n",
        "- MOONS: el coeficiente de silhouette para DBSCAN sube (ahora los clusters est√°n m√°s definidos que antes) y no se aprecian puntos ruido. \n",
        "- BLOBS: para todas las t√©cnicas aumentan el tiempo de ejecuci√≥n ligeramente aunque el coeficiente de silhouette es el mismo para todas ($\\sim 0.8$) \n",
        "Todas las t√©cnicas identificaron bien el n√∫mero de clusters y DBSCAN en comapraci√≥n al caso anterior, presenta una menor cantidad de puntos ruidos.\n",
        "- MUTATED: Nuevamente, solo GMM fue la √∫nica t√©cnica capaz de identificar tres clusters con las respectivas fronteras; las dem√°s t√©cnicas identifican tres clusters\n",
        "pero fallan en identificar las fronteras, donde MEANS Y WARD son poligonales y no siguen la forma el√≠ptica y DBSCAN identifica a duras penas un tercer cluster.\n",
        "\n",
        "## n_samples = 10000\n",
        "\n",
        "- MOONS: el coeficiente de silhouette para DBSCAN sube (ahora los clusters est√°n m√°s definidos que antes) y no se aprecian puntos ruido. Todos los m√©todos salvo KMEANS aumentaron el tiempo de ejecuci√≥n, donde WARD fue el que m√°s aumento su tiempo de ejecuci√≥n. Esto es porque este tipo de clustering (Aglomerativo)\n",
        "inicia donde cada punto es un √∫nico punto y de mnaera iterativa, se van fusionando los 2 clusters m√°s similares. De espera que a medida que aumentan la cantidad\n",
        "de puntos (5000, 10000), deber√≠a aumentar el tiempo de ejecuci√≥n para Aglomerative clustering, pero ahora se est√° usando WARD, donde cada vez que se fusionan\n",
        "los clusters, se debe calcular la media del cluster resultante, se actualiza la suma de cuadrados intra-cluster y se recalcula el incremento de varianza con\n",
        "todos los dem√°s clusters .\n",
        " \n",
        "- BLOBS: Todas las t√©cnicas aumentan el tiempo de ejecuci√≥n ligeramente aunque el coeficiente de silhouette es el mismo para todas ($\\sim 0.8$), salvo para DBSCAN\n",
        "donde ahora solo identifica 2 clusters y presenta un coeficiente de silhouette bajo ($\\sim$ 0.3)\n",
        "\n",
        "- MUTATED: Nuevamente, solo GMM fue la √∫nica t√©cnica capaz de identificar tres clusters. Los tiempos de ejecuci√≥n aumentaron levemente para todas las t√©cnicas, donde es m√°s notorio para WARD\n",
        "por el motivo anteriormente explicado.\n",
        "Ahora DBSCAN solo pudo identificar 2 clusters y ni siquiera identific√≥ puntos ruidos.\n",
        "\n",
        "Lo que se puede concluir al comparar con diferentes tama√±os de muestras y con clusters con diferentes formas es:\n",
        "\n",
        "- No fijarse solo en el coeficiente de silhouette: Hay casos donde la t√©cnica si pudo identificar bien el n√∫mero de clusters (DBSCAN -moons) pero presentaban un bajo coeficiente de Silhouette: En el caso donde hab√≠an m√°s clusters (BLOBS Y MUTATED) tambi√©n se espera obtener un alto coeficiente de silhouette, ya que por construcci√≥n, a medida\n",
        "que aumenta el n√∫mero de clusters, aumenta el coeficiente. Por lo que deber√≠an usarse otras m√©tricas para medir si el clustering realizado es bueno o no, como por ejemplo m√©tricas de cohesi√≥n, separaci√≥n, entre otras. Es m√°s, para DBSCAN, no deber√≠an usarse m√©tricas como la silhouette o de cohesi√≥n/divisi√≥n ya que estas se usan para clustering particional. Para DBSCAN est√° DBCV (density-based clustering validation)\n",
        "\n",
        "- Es preferible tener conocimiento de la estructura de los clusters, ya que hay algoritmos como KMEANS que son robustos solo a clusters esf√©ricos. Aunque se sabe que es d√≠ficil saber esto porque en la realidad, uno no dispone de las \"etiquetas\" de las diferentes clases presentes.\n",
        "\n",
        "Tambi√©n hay que tener cuidado con los hiperpar√°metros para cada modelo ya que estos afectan en el resultado del clustering,\n",
        "como por ejemplo KMEANS y GMM, que son sensibles a la inicializaci√≥n. DBSCAN tambi√©n es sensible por la elecci√≥n de eps y min_pts.\n",
        "- No usar algoritmos extremadamente caros computacionales si se tiene una gran muestra (caso WARD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13c5cb8067d9415f83b3d497954a437a",
        "deepnote_cell_type": "markdown",
        "id": "3mCbZc86rqC6"
      },
      "source": [
        "## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd6e991646b44f50a4b13f01d1542415",
        "deepnote_cell_type": "markdown",
        "id": "JI33m5jbrqC6"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media4.giphy.com/media/v1.Y2lkPTZjMDliOTUyb3B5Y3BtbTZwMnB0ZXRyejFpanJkNDl5cGhoeWlsc2k5bGx1MTUwYSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/l4FARHkIFJReGSy2c/giphy.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5742dfbd5a2e43778ff250436bab1005",
        "deepnote_cell_type": "markdown",
        "id": "h5k24znirqC7"
      },
      "source": [
        "Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Mathias* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:\n",
        "\n",
        "- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)\n",
        "- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)\n",
        "- *Age*: Edad actual de los pasajeros\n",
        "- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)\n",
        "- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)\n",
        "- *Flight distance*: Distancia del vuelo de este viaje\n",
        "- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)\n",
        "- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada\n",
        "- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea\n",
        "- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta\n",
        "- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida\n",
        "- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea\n",
        "- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento\n",
        "- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo\n",
        "- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo\n",
        "- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas\n",
        "- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje\n",
        "- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in\n",
        "- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo\n",
        "- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza\n",
        "- *Departure Delay in Minutes*: Minutos de retraso en la salida\n",
        "- *Arrival Delay in Minutes*: Minutos de retraso en la llegada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoIFHpw5xCW"
      },
      "source": [
        "En consideraci√≥n de lo anterior, realice las siguientes tareas:\n",
        "\n",
        "0. Ingeste el dataset a su ambiente de trabajo.\n",
        "\n",
        "1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]\n",
        "\n",
        "2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]\n",
        "\n",
        "3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]\n",
        "\n",
        "4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]\n",
        "\n",
        "5. De acuerdo con los resultados obtenidos en 4, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6tcVBCtxxS"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzHTZ17xveU_"
      },
      "outputs": [],
      "source": [
        "# 0) Carga de datos\n",
        "\n",
        "df = pd.read_parquet(\"aerolineas_lucer.parquet\")\n",
        "\n",
        "# 1) Selecci√≥n de variable num√©ricas - > el resto est√° desarrollado abajo\n",
        "\n",
        "num_df = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
        "num_df.info()\n",
        "\n",
        "# 2) Revisar distribuci√≥n varibles num√©ricas \n",
        "\n",
        "for col in num_df.columns:\n",
        "    fig = px.histogram(num_df, x=col, nbins=30, title=f\"Distribuci√≥n de {col}\")\n",
        "    fig.show()\n",
        "\n",
        "# 3) Abajo desarrollada\n",
        "\n",
        "# 4) Correlograma\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# PLotly\n",
        "corr = num_df.corr()\n",
        "fig = px.imshow(corr,\n",
        "                text_auto=True,\n",
        "                color_continuous_scale=\"RdBu_r\",\n",
        "                title=\"Matriz de correlaci√≥n\")\n",
        "fig.show()\n",
        "\n",
        "# Sin plotly\n",
        "# calcular matriz de correlaci√≥n\n",
        "corr = num_df.corr()\n",
        "\n",
        "# tama√±o de figura\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "# heatmap con seaborn\n",
        "sns.heatmap(corr,\n",
        "            annot=True,        # muestra los valores\n",
        "            cmap=\"RdBu_r\",     # misma paleta que usaste en Plotly\n",
        "            center=0,          # centro en 0 para ver bien positivos/negativos\n",
        "            fmt=\".2f\")         # redondeo a 2 decimales\n",
        "\n",
        "plt.title(\"Matriz de correlaci√≥n\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# 5) Abajo desarrollada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**DESARROLLO**\n",
        "\n",
        "1) **Inclusi√≥n de variables categor√≠cas en estos modelos**\n",
        "\n",
        "En un an√°lisis no supervisado, lo ideal es trabajar √∫nicamente con variables num√©ricas, ya que la mayor√≠a de los algoritmos, como K-means, PCA o m√©todos jer√°rquicos, se basan en la distancia euclidiana u otras m√©tricas equivalentes. Estas distancias est√°n definidas en un espacio continuo y requieren que las variables tengan una escala num√©rica coherente. La inclusi√≥n de variables categ√≥ricas sin un tratamiento adecuado distorsiona las distancias. Por ejemplo, codificar el g√©nero como ‚Äú0‚Äù y ‚Äú1‚Äù implica una relaci√≥n ordinal inexistente, mientras que aplicar one-hot encoding multiplica las dimensiones y puede sobrerrepresentar una sola caracter√≠stica respecto a las dem√°s. Como consecuencia, los clusters pueden formarse de manera artificial, reflejando diferencias de codificaci√≥n y no patrones reales en los datos. Por este motivo, en este tipo de an√°lisis no supervisado se recomienda trabajar con variables num√©ricas y, en caso de querer incorporar variables categ√≥ricas, transformarlas con t√©cnicas apropiadas que preserven su naturaleza, evitando que dominen o sesguen el c√°lculo de distancias.\n",
        "\n",
        "2) **Analice cada una de las distribuciones que se graficaron** \n",
        "\n",
        "- id: Distribuci√≥n completamente uniforme, ya que corresponde a un identificador √∫nico. No aporta informaci√≥n estad√≠stica √∫til para el an√°lisis de satisfacci√≥n y deber√≠a excluirse en etapas posteriores.\n",
        "\n",
        "- Age: Distribuci√≥n aproximadamente unimodal, concentrada entre los 20 y 50 a√±os, con un descenso progresivo en edades mayores. La mayor parte de los pasajeros pertenece a un rango etario adulto en edad laboral, lo cual se alinea con viajes de negocios.\n",
        "\n",
        "- Flight Distance: Distribuci√≥n fuertemente sesgada a la derecha. La mayor√≠a de los vuelos tiene distancias cortas a medias (menores a 2000 km), con pocos vuelos de larga distancia que act√∫an como valores at√≠picos.\n",
        "\n",
        "- Inflight wifi service: Distribuci√≥n multimodal, con concentraciones en los niveles 1, 2 y 3 de satisfacci√≥n. Se observa que una fracci√≥n importante de pasajeros calific√≥ el servicio como bajo, aunque tambi√©n existe un grupo relevante que lo valor√≥ positivamente en nivel 4 o 5.\n",
        "\n",
        "- Departure/Arrival time convenient: Distribuci√≥n m√°s equilibrada, con valores relevantes en los extremos (1 y 5), lo que sugiere percepciones polarizadas: algunos consideran los horarios poco convenientes, mientras que otros los califican como muy adecuados.\n",
        "\n",
        "- Ease of Online booking: Distribuci√≥n centrada en los niveles 2, 3 y 4, con relativamente menos pasajeros en 0 y 5. Esto indica una percepci√≥n moderada de la facilidad de reserva en l√≠nea, sin concentrarse en los extremos.\n",
        "\n",
        "- Gate location: Distribuci√≥n sesgada hacia niveles intermedios (3 y 4). Esto refleja que, en general, la ubicaci√≥n de las puertas es percibida como aceptable o buena, pero no alcanza a concentrarse en el nivel m√°ximo (5).\n",
        "\n",
        "- Food and drink: Distribuci√≥n con una clara tendencia hacia valores altos (4 y 5). Indica que la mayor√≠a de los pasajeros se encuentra satisfecha con este aspecto del servicio.\n",
        "\n",
        "- Online boarding: Distribuci√≥n con sesgo positivo: gran parte de los pasajeros calific√≥ entre 3 y 5, con un pico marcado en 4. Refleja una buena percepci√≥n del proceso de embarque en l√≠nea.\n",
        "\n",
        "- Seat comfort: Distribuci√≥n bimodal: algunos pasajeros califican bajo (1 y 2), mientras que un grupo mayor lo valora alto (4 y 5). Esto refleja diferencias en experiencias seg√∫n clase de viaje o expectativas del pasajero.\n",
        "\n",
        "- Inflight entertainment: Distribuci√≥n con clara tendencia hacia niveles altos (4 y 5), aunque a√∫n hay un grupo no menor que calific√≥ con 1 o 2. Esto indica que, en general, los pasajeros est√°n satisfechos con el entretenimiento a bordo, pero existe un segmento insatisfecho.\n",
        "\n",
        "- On-board service: Predominio de valoraciones positivas (4 y 5), con una menor proporci√≥n de valoraciones bajas. Refleja una buena percepci√≥n general del servicio brindado por la tripulaci√≥n durante el vuelo.\n",
        "\n",
        "- Leg room service: Distribuci√≥n sesgada a valores altos (4 y 5), pero con presencia visible de calificaciones bajas (1 y 2). Sugiere que aunque la mayor√≠a considera aceptable el espacio para las piernas, un grupo considerable lo percibe insuficiente.\n",
        "\n",
        "- Baggage handling: Distribuci√≥n altamente concentrada en valores de 4 y 5, con menor proporci√≥n en 1 y 2. Esto muestra que el manejo de equipaje es uno de los aspectos mejor evaluados en general.\n",
        "\n",
        "- Check-in service: La mayor√≠a de los pasajeros asigna puntuaciones intermedias y altas (3, 4 y 5). Indica que el proceso de check-in es percibido como eficiente en la mayor√≠a de los casos, con pocas valoraciones negativas.\n",
        "\n",
        "- Inflight service: Distribuci√≥n claramente inclinada hacia niveles 4 y 5, lo que refleja una buena percepci√≥n del servicio recibido durante el vuelo. Pocos pasajeros lo evaluaron con puntuaciones bajas.\n",
        "\n",
        "- Cleanliness: La limpieza de los aviones presenta una distribuci√≥n favorable, con predominio en valores de 4 y 5. Este patr√≥n indica que los pasajeros valoran positivamente este aspecto, aunque existen algunas evaluaciones negativas.\n",
        "\n",
        "- Departure Delay in Minutes: Distribuci√≥n fuertemente sesgada a la derecha, con la gran mayor√≠a de los vuelos presentando retrasos bajos (cercanos a 0 minutos). Sin embargo, se observan valores extremos (outliers) de hasta m√°s de 1000 minutos, lo cual refleja la presencia de vuelos excepcionalmente retrasados.\n",
        "\n",
        "- Arrival Delay in Minutes: Distribuci√≥n muy similar a la de retrasos en la salida: la mayor√≠a de los vuelos tiene poco o ning√∫n retraso a la llegada, pero existen casos extremos de gran magnitud. Esto es consistente con la correlaci√≥n esperada entre ambas variables (si un vuelo sale tarde, suele llegar tarde).\n",
        "\n",
        "3) **Necesidad de escalar datos**\n",
        "\n",
        "Al analizar los gr√°ficos de distribuci√≥n se observa que las variables del dataset se encuentran en escalas muy diferentes. Por ejemplo, las variables de satisfacci√≥n asociadas a servicios en vuelo o procesos en tierra se miden en escalas discretas de 0 a 5, mientras que otras como Flight Distance pueden alcanzar valores del orden de miles de kil√≥metros y las variables de retrasos (Departure Delay in Minutes, Arrival Delay in Minutes) presentan valores que van desde cero hasta m√°s de mil minutos en casos extremos. Esta heterogeneidad en las escalas implica que, si no se realiza un proceso de estandarizaci√≥n, las variables con mayor rango num√©rico dominar√°n el c√°lculo de distancias en los algoritmos no supervisados, desplazando la influencia de aquellas que se encuentran en escalas m√°s reducidas. Dado que la mayor√≠a de las t√©cnicas de clustering y reducci√≥n de dimensionalidad se basan en m√©tricas de distancia eucl√≠dea, resulta necesario aplicar un m√©todo de escalamiento como StandardScaler o RobustScaler, de modo que todas las variables contribuyan de manera equilibrada al an√°lisis y no se pierda informaci√≥n relevante debido a la disparidad en sus magnitudes.\n",
        "\n",
        "4) **An√°lisis de correlaciones**\n",
        "\n",
        "Al examinar el correlograma se identifican varios patrones relevantes de asociaci√≥n entre las variables. En primer lugar, se observa una correlaci√≥n muy alta entre Departure Delay in Minutes y Arrival Delay in Minutes (r ‚âà 0.97), lo que era esperable ya que los retrasos en la salida se propagan directamente hacia la llegada. Esta relaci√≥n indica que ambas variables son pr√°cticamente redundantes y contienen informaci√≥n similar.\n",
        "\n",
        "En segundo lugar, se aprecia un bloque de correlaciones positivas moderadas y fuertes entre las variables relacionadas con la experiencia del pasajero en vuelo, como Seat comfort, Inflight entertainment, On-board service, Leg room service, Cleanliness y Food and drink. Esto sugiere que los pasajeros que eval√∫an bien un aspecto del servicio tienden a hacerlo tambi√©n en otros, reflejando una percepci√≥n global de calidad. Dentro de este grupo, destacan las correlaciones entre Seat comfort y Cleanliness (r ‚âà 0.68), as√≠ como entre Food and drink y Seat comfort (r ‚âà 0.57).\n",
        "\n",
        "Por otro lado, variables como Flight Distance y Age presentan correlaciones muy bajas o pr√°cticamente nulas con la mayor√≠a de las dem√°s, lo que indica que aportan informaci√≥n independiente y no redundante respecto a la satisfacci√≥n del servicio. Algo similar ocurre con Departure/Arrival time convenient y Gate location, que mantienen correlaciones d√©biles con el resto de variables.\n",
        "\n",
        "Finalmente, se observa que Inflight wifi service y Ease of Online booking tienen correlaciones positivas moderadas con Online boarding y otras variables de servicio (r entre 0.4 y 0.7), lo cual sugiere que la facilidad tecnol√≥gica influye en la percepci√≥n de calidad general del vuelo.\n",
        "\n",
        "Importante tener en mente que el correlograma permite identificar grupos de variables redundantes (especialmente retrasos y servicios a bordo) y otras variables que aportan informaci√≥n diferenciada (edad, distancia del vuelo, conveniencia de horarios), lo cual ser√° clave para tomar decisiones en la reducci√≥n de dimensionalidad. Pero nunca olvidar que correlaci√≥n no significa causalidad, tambi√©n pueden haber casualidades.\n",
        "\n",
        "5) **Reducci√≥n del df**\n",
        "\n",
        "De acuerdo con los resultados del correlograma, se seleccionaron cuatro variables no redundantes y representativas de distintas dimensiones de la experiencia de vuelo. Se eligi√≥ *Seat comfort* como variable representativa del conjunto de factores de satisfacci√≥n en vuelo, dado que presenta altas correlaciones con aspectos como Food and drink, Leg room service, Cleanliness y On-board service, por lo que conservar solo esta variable permite captar la percepci√≥n global de calidad en el servicio f√≠sico sin mantener redundancias. Se incluy√≥ *Inflight wifi service* porque refleja la dimensi√≥n tecnol√≥gica de la experiencia, muestra correlaciones moderadas con variables como Ease of Online booking y Online boarding y no est√° fuertemente relacionada con los factores de confort, aportando informaci√≥n diferenciada. Se mantuvo *Flight distance* debido a que presenta correlaciones muy bajas con el resto de las variables y aporta informaci√≥n independiente asociada a la naturaleza objetiva del viaje. Finalmente, entre Departure Delay in Minutes y *Arrival Delay in Minutes*, que muestran una correlaci√≥n muy alta, se conserv√≥ la segunda por representar de manera m√°s directa el impacto del retraso percibido por el pasajero. Con esto, las cuatro variables seleccionadas fueron **Seat comfort, Inflight wifi service, Flight distance y Arrival Delay in Minutes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4b6c047d994f40ea9e78a36a777042e0",
        "deepnote_cell_type": "markdown",
        "id": "PNGfTgtkrqC9"
      },
      "source": [
        "## 3. Preprocesamiento üé≠. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "713b3f0e61dd4841bb5b38c730d344d5",
        "deepnote_cell_type": "markdown",
        "id": "6RZD0fMNrqC-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media.tenor.com/R_WseIIwQ8QAAAAM/beavis-computer.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98400c7b5fec4af193eec3601f53891e",
        "deepnote_cell_type": "markdown",
        "id": "J6d4VEOTrqC-"
      },
      "source": [
        "Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.\n",
        "\n",
        "Bas√°ndose en los an√°lisis realizados anteriormente:\n",
        "1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]\n",
        "2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDSaGoq0OUp"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ad1e70818ad748638ca0927b07a76125",
        "deepnote_cell_type": "code",
        "id": "gBYG238wrqC-"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# 1\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Selecci√≥n de las 4 variables representativas\n",
        "features = [\"Seat comfort\", \"Inflight wifi service\", \n",
        "            \"Flight Distance\", \"Arrival Delay in Minutes\"]\n",
        "\n",
        "X = df[features].dropna()\n",
        "\n",
        "# Definir pipeline: escalamiento + PCA\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),   # normaliza variables\n",
        "    (\"pca\", PCA(n_components=2, random_state=42))  # reduce a 2D\n",
        "])\n",
        "\n",
        "# Aplicar pipeline\n",
        "X_pca = pipeline.fit_transform(X)\n",
        "\n",
        "# Guardar en DataFrame para graficar\n",
        "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
        "pca_df.head()\n",
        "\n",
        "\n",
        "# 2\n",
        "import plotly.express as px\n",
        "\n",
        "#2D\n",
        "fig = px.scatter(\n",
        "    pca_df, x=\"PC1\", y=\"PC2\",\n",
        "    opacity=0.6,\n",
        "    title=\"Proyecci√≥n PCA a 2 dimensiones\"\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "#3D\n",
        "pca3 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=3, random_state=42))\n",
        "])\n",
        "X_pca3 = pca3.fit_transform(X)\n",
        "\n",
        "pca3_df = pd.DataFrame(X_pca3, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
        "\n",
        "fig3d = px.scatter_3d(\n",
        "    pca3_df, x=\"PC1\", y=\"PC2\", z=\"PC3\",\n",
        "    opacity=0.6,\n",
        "    title=\"Proyecci√≥n PCA a 3 dimensiones\"\n",
        ")\n",
        "fig3d.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comentarios**\n",
        "La proyecci√≥n a dos y tres dimensiones mediante PCA muestra que gran parte de las observaciones se concentran en una regi√≥n reducida del espacio transformado, con una alta densidad en torno al centro y la aparici√≥n de algunos valores at√≠picos que se dispersan hacia los extremos de los componentes principales. Esto indica que, aunque los datos originales conten√≠an m√∫ltiples variables con distintos rangos y escalas, la transformaci√≥n logr√≥ condensar la informaci√≥n en un n√∫mero reducido de ejes que capturan la mayor parte de la variabilidad.\n",
        "\n",
        "En la proyecci√≥n bidimensional se aprecia un patr√≥n alargado con gradiente de dispersi√≥n, lo que sugiere que la varianza de los datos est√° dominada por una combinaci√≥n de variables espec√≠ficas (en este caso, aquellas asociadas a retrasos y distancia de vuelo). La representaci√≥n tridimensional refuerza esta interpretaci√≥n, mostrando c√≥mo los individuos se organizan a lo largo de un eje principal de variaci√≥n, mientras que los otros dos ejes aportan menor diferenciaci√≥n.\n",
        "\n",
        "En t√©rminos pr√°cticos, esta visualizaci√≥n confirma la utilidad de la reducci√≥n de dimensionalidad pues los datos, que en su forma original eran complejos y redundantes, pueden ser representados en un espacio reducido que facilita la detecci√≥n de patrones, agrupamientos o comportamientos at√≠picos. No obstante, tambi√©n revela que la estructura intr√≠nseca de los datos no se separa claramente en clusters definidos, lo que anticipa que, para un an√°lisis de agrupamiento, podr√≠an requerirse t√©cnicas adicionales (como algoritmos de clustering o el uso de componentes adicionales) para obtener segmentaciones m√°s significativas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd281470d3054764a63d857cfa7d52a6",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "7ENoOtIIrqC_"
      },
      "source": [
        "## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db89e9c9f35c44abbd8991180226c0ea",
        "deepnote_cell_type": "markdown",
        "id": "fbGw6Sa-rqC_"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif\" width=250>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3e2f59fa12954641af7a854a4e203694",
        "deepnote_cell_type": "markdown",
        "id": "nl_ccu9brqDA"
      },
      "source": [
        "Con el objetivo de mantener la claridad en su an√°lisis, Don Mathias le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.\n",
        "\n",
        "1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]\n",
        "\n",
        "2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]\n",
        "\n",
        "3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cS1FR00NlF"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "be86896911244aa89e3b5f3f00a286af",
        "deepnote_cell_type": "code",
        "id": "iaPZFmjyrqDA"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# 1\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Variables seleccionadas en el an√°lisis previo\n",
        "features = [\"Seat comfort\", \"Inflight wifi service\", \n",
        "            \"Flight Distance\", \"Arrival Delay in Minutes\"]\n",
        "\n",
        "# Pipeline: escalado + modelo de outliers\n",
        "pipe_outliers = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"iso_forest\", IsolationForest(contamination=0.01, random_state=42))\n",
        "])\n",
        "\n",
        "# Entrenar y predecir anomal√≠as\n",
        "y_pred = pipe_outliers.fit_predict(df[features])\n",
        "\n",
        "# Agregar resultados al dataframe\n",
        "df[\"anomaly\"] = y_pred\n",
        "# En IsolationForest: -1 = an√≥malo, 1 = normal\n",
        "\n",
        "\n",
        "# 2\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reducir a 2 componentes para graficar\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(StandardScaler().fit_transform(df[features]))\n",
        "\n",
        "df[\"PC1\"] = X_pca[:, 0]\n",
        "df[\"PC2\"] = X_pca[:, 1]\n",
        "\n",
        "# Graficar, diferenciando outliers\n",
        "fig = px.scatter(\n",
        "    df, x=\"PC1\", y=\"PC2\",\n",
        "    color=df[\"anomaly\"].map({1: \"Normal\", -1: \"An√≥malo\"}),\n",
        "    title=\"Detecci√≥n de outliers con IsolationForest (PCA 2D)\",\n",
        "    opacity=0.7\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comentario** El rendimiento del modelo se eval√∫a considerando que no existen etiquetas verdaderas para anomal√≠as, por lo que no es posible aplicar m√©tricas supervisadas como precisi√≥n o recall. El IsolationForest se configur√≥ para identificar al 1% de los pasajeros como an√≥malos, lo que es coherente con la expectativa de que los casos at√≠picos sean minoritarios. Al proyectar los resultados en el espacio reducido mediante PCA, los pasajeros marcados como an√≥malos se ubican mayoritariamente en la periferia de la nube de datos, lo que concuerda con la definici√≥n de outlier como un individuo alejado del patr√≥n central. Adem√°s, muchos de estos casos corresponden a valores extremos en variables como retrasos de m√°s de mil minutos o distancias de vuelo inusuales, lo que refuerza la validez del modelo. Por tanto, aunque no puede evaluarse con m√©tricas cl√°sicas, el modelo se considera adecuado porque identifica correctamente observaciones extremas y consistentes con el dominio del problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3871e2fe5bdd422dbdbfaebf75503ae3",
        "deepnote_cell_type": "markdown",
        "id": "zQFTklmVrqDB"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "236333de6dd445c182aefcc507589325",
        "deepnote_cell_type": "markdown",
        "id": "YpNj4wbPrqDB"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.imgflip.com/6xz0ij.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7e1ceb91be94b1da2ab8be97dfac999",
        "deepnote_cell_type": "markdown",
        "id": "CR3hzRxrrqDB"
      },
      "source": [
        "Motivado por incrementar su fortuna, Don Mathias le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:\n",
        "\n",
        "1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]\n",
        "2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]\n",
        "\n",
        "> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_T_zTg0MXB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "6d3d1bb3fda14321984466d9101a775a",
        "deepnote_cell_type": "code",
        "id": "5GeUb9J3rqDB"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# 1\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Pipeline con escalamiento y PCA\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=4)),  # para seguir con la tendencia previa\n",
        "])\n",
        "\n",
        "X_proc = pipeline.fit_transform(X)  # usar las 4 variables seleccionadas\n",
        "\n",
        "# Guardamos resultados\n",
        "aic = []\n",
        "bic = []\n",
        "n_clusters = range(3, 9)\n",
        "\n",
        "for k in n_clusters:\n",
        "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "    gmm.fit(X_proc)\n",
        "    aic.append(gmm.aic(X_proc))\n",
        "    bic.append(gmm.bic(X_proc))\n",
        "\n",
        "# 2\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(n_clusters, aic, marker=\"o\", label=\"AIC\")\n",
        "plt.plot(n_clusters, bic, marker=\"o\", label=\"BIC\")\n",
        "plt.xlabel(\"N√∫mero de cl√∫sters\")\n",
        "plt.ylabel(\"Valor del criterio\")\n",
        "plt.title(\"Comparaci√≥n de AIC y BIC en Gaussian Mixture\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comenntario** El criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters en un modelo Gaussian Mixture es comparar los valores de AIC (Akaike Information Criterion) y BIC (Bayesian Information Criterion). Ambos indicadores buscan balancear la calidad del ajuste y la complejidad del modelo, penalizando el exceso de par√°metros para evitar sobreajuste. En particular, el BIC aplica una penalizaci√≥n m√°s estricta que el AIC, por lo que suele ser m√°s conservador en la elecci√≥n del n√∫mero de cl√∫sters.\n",
        "\n",
        "Al analizar los resultados gr√°ficos, se observa que tanto AIC como BIC disminuyen consistentemente a medida que aumenta el n√∫mero de cl√∫sters de 3 a 8. Estad√≠sticamente, el n√∫mero √≥ptimo de cl√∫sters corresponde al punto en que estos criterios alcanzan su valor m√≠nimo. En este caso, el valor m√°s bajo de ambos criterios se obtiene con 8 cl√∫sters, lo que indica que este es el modelo con mejor equilibrio entre ajuste y complejidad.\n",
        "\n",
        "Por lo tanto, la justificaci√≥n estad√≠stica y gr√°fica sugiere seleccionar 8 cl√∫sters como el n√∫mero √≥ptimo para segmentar a los pasajeros, ya que este modelo maximiza la parsimonia seg√∫n AIC y BIC y captura de manera m√°s adecuada la estructura latente de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dd342e336254418ba766b29dce16b267",
        "deepnote_cell_type": "markdown",
        "id": "P9CERnaerqDC"
      },
      "source": [
        "## 6. An√°lisis de resultados üìä [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "953b5ad01a704b50b899db7176d1b7b2",
        "deepnote_cell_type": "markdown",
        "id": "I1yNa111rqDC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5b/03/4e/5b034e96d84c6c6b57a9a04ca14aac02.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd90e2f135404353ac0b5ab844936ca7",
        "deepnote_cell_type": "markdown",
        "id": "dg0Qx4RZrqDC"
      },
      "source": [
        "Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:\n",
        "\n",
        "1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]\n",
        "\n",
        "2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]\n",
        "\n",
        "3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]\n",
        "\n",
        "4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]\n",
        "\n",
        "5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRN0zZip0IMB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9abf4dbc643e40cebe99fcb1ff3ff413",
        "deepnote_cell_type": "code",
        "id": "XmZrz15GrqDC"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8igIgDwpq9mG"
      },
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\" width=300>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7cb425aec99b4079954fd707109c42c3",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-26T06:15:51.197Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
